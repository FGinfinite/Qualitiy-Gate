#!/usr/bin/env python3
"""
Select-MoEæ•°æ®é€‰æ‹©ç»¼åˆå¯è§†åŒ–åˆ†æè„šæœ¬

æœ¬è„šæœ¬æä¾›äº†å¯¹Select-MoEæ•°æ®é€‰æ‹©è¿‡ç¨‹çš„å…¨é¢åˆ†æå’Œå¯è§†åŒ–ï¼ŒåŒ…æ‹¬ï¼š
1. è´¨é‡é—¨åˆ†æï¼šå±•ç¤ºæ ·æœ¬çš„è´¨é‡åˆ†æ•°åˆ†å¸ƒ
2. MoEè·¯ç”±åˆ†æï¼šåˆ†æä¸“å®¶é€‰æ‹©æ¨¡å¼
3. é€å±‚äºŒçº§è·¯ç”±ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼šæ ·æœ¬é—´ç›¸ä¼¼æ€§åº¦é‡
4. FPSç®—æ³•åº”ç”¨ï¼šå¤šæ ·æ€§é€‰æ‹©è¿‡ç¨‹
5. é€‰æ‹©ç»“æœå¯¹æ¯”ï¼šè´¨é‡é€‰æ‹© vs å¤šæ ·æ€§é€‰æ‹©

è¿™ä¸ªè„šæœ¬å¸®åŠ©ç†è§£æ•´ä¸ªSelect-MoEæ•°æ®é€‰æ‹©ç®¡é“çš„å·¥ä½œåŸç†ã€‚
æ”¯æŒè¯»å–æ•´ä¸ªrouter_dataæ–‡ä»¶å¤¹å†…çš„æ‰€æœ‰.ptæ–‡ä»¶ï¼Œå¹¶å¯¹æ¯ä¸ªæ•°æ®é›†è¿›è¡Œåˆ†æã€‚
"""

import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import argparse
import glob

import matplotlib.pyplot as plt
import numpy as np
import torch

try:
    from cuml.manifold import TSNE as CumlTSNE

    CUML_AVAILABLE = True
except ImportError:
    CUML_AVAILABLE = False

from sklearn.manifold import TSNE

from src.clustering import ClusterBasedSelection
from src.stages.selection import load_router_data


def optimize_tsne_parameters(n_samples, n_features):
    """
    æ ¹æ®æ•°æ®é›†å¤§å°æ™ºèƒ½é€‰æ‹©t-SNEå‚æ•°

    Args:
        n_samples: æ ·æœ¬æ•°
        n_features: ç‰¹å¾æ•°

    Returns:
        dict: ä¼˜åŒ–çš„å‚æ•°å­—å…¸
    """
    # æ ¹æ®æ•°æ®é›†å¤§å°è°ƒæ•´å‚æ•°
    if n_samples < 1000:
        # å°æ•°æ®é›†ï¼šä½¿ç”¨ç²¾ç¡®æ–¹æ³•ï¼Œæ›´é«˜è´¨é‡
        return {"method": "exact", "n_iter": 1500, "perplexity": min(30, n_samples // 3), "early_exaggeration": 12.0, "learning_rate": "auto"}
    elif n_samples < 5000:
        # ä¸­ç­‰æ•°æ®é›†ï¼šå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡
        return {"method": "barnes_hut", "n_iter": 1000, "perplexity": min(50, n_samples // 4), "early_exaggeration": 12.0, "learning_rate": "auto"}
    else:
        # å¤§æ•°æ®é›†ï¼šä¼˜å…ˆé€Ÿåº¦
        return {
            "method": "barnes_hut",
            "n_iter": 750,  # å‡å°‘è¿­ä»£æ¬¡æ•°åŠ é€Ÿ
            "perplexity": min(50, max(10, n_samples // 10)),
            "early_exaggeration": 8.0,  # å‡å°æ—©æœŸæ”¾å¤§ç³»æ•°
            "learning_rate": 200.0,  # å›ºå®šå­¦ä¹ ç‡åŠ é€Ÿæ”¶æ•›
        }


def gpu_accelerated_tsne(features, n_components=2, perplexity=30, random_state=42, use_gpu=True):
    """
    GPUåŠ é€Ÿçš„t-SNEå®ç°ï¼Œè‡ªåŠ¨å›é€€åˆ°CPUç‰ˆæœ¬

    Args:
        features: è¾“å…¥ç‰¹å¾çŸ©é˜µ (numpy array)
        n_components: è¾“å‡ºç»´åº¦
        perplexity: t-SNE perplexityå‚æ•°
        random_state: éšæœºç§å­
        use_gpu: æ˜¯å¦å°è¯•ä½¿ç”¨GPUåŠ é€Ÿ

    Returns:
        numpy array: é™ç»´åçš„åæ ‡
    """
    n_samples, n_features = features.shape

    # ä¼˜åŒ–å‚æ•°
    params = optimize_tsne_parameters(n_samples, n_features)
    adjusted_perplexity = min(perplexity, params["perplexity"])

    # å°è¯•GPUåŠ é€Ÿçš„cuML t-SNE
    if use_gpu and CUML_AVAILABLE:
        try:
            # cuML t-SNEå‚æ•°
            tsne_gpu = CumlTSNE(
                n_components=n_components,
                perplexity=adjusted_perplexity,
                random_state=random_state,
                learning_rate=params["learning_rate"],
                n_iter=params["n_iter"],
                early_exaggeration=params["early_exaggeration"],
                method=params["method"],
            )

            coords_2d = tsne_gpu.fit_transform(features)

            # cuMLè¿”å›çš„å¯èƒ½æ˜¯cupyæ•°ç»„ï¼Œè½¬æ¢ä¸ºnumpy
            if hasattr(coords_2d, "get"):
                coords_2d = coords_2d.get()  # cupy to numpy
            elif hasattr(coords_2d, "cpu"):
                coords_2d = coords_2d.cpu().numpy()  # torch to numpy

            return coords_2d

        except Exception as e:
            print(f"GPU t-SNEå¤±è´¥ï¼Œå›é€€åˆ°CPUç‰ˆæœ¬: {str(e)}")

    # CPUç‰ˆæœ¬çš„sklearn t-SNEï¼ˆå›é€€æ–¹æ¡ˆï¼‰
    # æ³¨æ„: sklearn t-SNEä¸æ”¯æŒlearning_rate='auto'å’Œmethodå‚æ•°
    learning_rate = 200.0 if params["learning_rate"] == "auto" else params["learning_rate"]
    method = "barnes_hut" if params["method"] == "barnes_hut" else "exact"

    tsne_cpu = TSNE(
        n_components=n_components,
        perplexity=adjusted_perplexity,
        random_state=random_state,
        n_iter=params["n_iter"],
        learning_rate=learning_rate,
        method=method,
    )

    coords_2d = tsne_cpu.fit_transform(features)
    return coords_2d


plt.rcParams["font.sans-serif"] = ["Maple Mono NF CN"]


def analyze_quality_gates(router_data, top_k=10):
    """åˆ†æè´¨é‡é—¨çš„è¾“å‡ºåˆ†å¸ƒ"""
    quality_score = router_data["quality_score"]  # [N, L, 1]
    sample_ids = router_data["sample_ids"]

    print("=" * 70)
    print("è´¨é‡é—¨åˆ†æ")
    print("=" * 70)

    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„è´¨é‡åˆ†æ•°ï¼ˆä½¿ç”¨sigmoidè½¬æ¢ï¼‰
    quality_probs = torch.sigmoid(quality_score.float())  # [N, L, 1]
    good_probs = quality_probs.squeeze(-1)  # [N, L] - goodæ¦‚ç‡

    # æ¯ä¸ªæ ·æœ¬åœ¨æ‰€æœ‰å±‚çš„å¹³å‡goodæ¦‚ç‡ä½œä¸ºè´¨é‡åˆ†æ•°
    sample_quality_scores = good_probs.mean(dim=1).numpy()  # [N]

    print("è´¨é‡åˆ†æ•°ç»Ÿè®¡:")
    print(f"  å¹³å‡å€¼: {sample_quality_scores.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {sample_quality_scores.std():.4f}")
    print(f"  æœ€å°å€¼: {sample_quality_scores.min():.4f}")
    print(f"  æœ€å¤§å€¼: {sample_quality_scores.max():.4f}")
    print()

    # æ‰¾å‡ºè´¨é‡æœ€é«˜å’Œæœ€ä½çš„æ ·æœ¬
    sorted_indices = np.argsort(sample_quality_scores)

    print(f"è´¨é‡æœ€ä½çš„{top_k}ä¸ªæ ·æœ¬:")
    for i in range(min(top_k, len(sorted_indices))):
        idx = sorted_indices[i]
        print(f"  {sample_ids[idx]}: {sample_quality_scores[idx]:.4f}")
    print()

    print(f"è´¨é‡æœ€é«˜çš„{top_k}ä¸ªæ ·æœ¬:")
    for i in range(min(top_k, len(sorted_indices))):
        idx = sorted_indices[-(i + 1)]
        print(f"  {sample_ids[idx]}: {sample_quality_scores[idx]:.4f}")
    print()

    return sample_quality_scores, sorted_indices


def analyze_moe_routing(router_data):
    """åˆ†æMoEè·¯ç”±æ¨¡å¼"""
    moe_logits = router_data["moe_logits"]  # [N, L, E] - å·²ç»æ˜¯æ¦‚ç‡åˆ†å¸ƒ

    print("=" * 70)
    print("MoEè·¯ç”±åˆ†æ")
    print("=" * 70)

    # æ•°æ®å·²ç»æ˜¯æ¦‚ç‡åˆ†å¸ƒæ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
    moe_probs = moe_logits.float()  # [N, L, E]
    n_samples, n_layers, n_experts = moe_probs.shape

    print(f"æ•°æ®å½¢çŠ¶: {n_samples} æ ·æœ¬ Ã— {n_layers} å±‚ Ã— {n_experts} ä¸“å®¶")
    print()

    # é€å±‚è®¡ç®—ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡å’Œå¹³è¡¡åº¦
    layer_expert_usage = []  # æ¯å±‚çš„ä¸“å®¶ä½¿ç”¨ç‡
    layer_balance_scores = []  # æ¯å±‚çš„å¹³è¡¡åº¦
    layer_entropy_stats = []  # æ¯å±‚çš„ç†µç»Ÿè®¡

    print("å„å±‚ä¸“å®¶ä½¿ç”¨å’Œå¹³è¡¡åº¦åˆ†æ:")
    print(f"{'å±‚':<4} {'å¹³å‡ä½¿ç”¨ç‡':<10} {'ä½¿ç”¨ç‡æ ‡å‡†å·®':<12} {'å¹³è¡¡åº¦':<8} {'å¹³å‡ç†µ':<8} {'ç†µæ ‡å‡†å·®':<8}")
    print("-" * 60)

    for layer_idx in range(n_layers):
        # å½“å‰å±‚çš„æ•°æ®: [N, E]
        layer_probs = moe_probs[:, layer_idx, :]  # [N, E]

        # 1. è®¡ç®—å½“å‰å±‚çš„ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
        layer_expert_usage_rates = layer_probs.mean(dim=0).numpy()  # [E] - å½“å‰å±‚æ¯ä¸ªä¸“å®¶çš„å¹³å‡ä½¿ç”¨ç‡
        layer_expert_usage.append(layer_expert_usage_rates)

        # 2. è®¡ç®—å½“å‰å±‚çš„å¹³è¡¡åº¦
        layer_balance = 1 - layer_expert_usage_rates.std() * len(layer_expert_usage_rates)
        layer_balance_scores.append(layer_balance)

        # 3. è®¡ç®—å½“å‰å±‚æ¯ä¸ªæ ·æœ¬çš„è·¯ç”±ç†µ
        layer_entropy_per_sample = -torch.sum(layer_probs * torch.log(layer_probs + 1e-8), dim=-1).numpy()  # [N]
        layer_mean_entropy = layer_entropy_per_sample.mean()
        layer_std_entropy = layer_entropy_per_sample.std()
        layer_entropy_stats.append({"mean": layer_mean_entropy, "std": layer_std_entropy, "per_sample": layer_entropy_per_sample})

        # è¾“å‡ºå½“å‰å±‚çš„ç»Ÿè®¡ä¿¡æ¯
        print(
            f"{layer_idx:<4} {layer_expert_usage_rates.mean():<10.4f} {layer_expert_usage_rates.std():<12.4f} "
            f"{layer_balance:<8.4f} {layer_mean_entropy:<8.4f} {layer_std_entropy:<8.4f}"
        )

    print()

    # è®¡ç®—æ•´ä½“ç»Ÿè®¡ï¼ˆå„å±‚å¹³å‡ï¼‰
    overall_balance = np.mean(layer_balance_scores)
    overall_entropy_means = [stats["mean"] for stats in layer_entropy_stats]
    overall_avg_entropy = np.mean(overall_entropy_means)
    overall_entropy_std = np.std(overall_entropy_means)  # å±‚é—´ç†µçš„æ ‡å‡†å·®

    # è®¡ç®—æ¯ä¸ªæ ·æœ¬åœ¨æ‰€æœ‰å±‚çš„å¹³å‡ç†µ
    sample_avg_entropy = np.mean([stats["per_sample"] for stats in layer_entropy_stats], axis=0)  # [N]

    # æ•´åˆæ‰€æœ‰å±‚çš„ä¸“å®¶ä½¿ç”¨ç‡ï¼ˆç”¨äºå¯è§†åŒ–å’Œå…¶ä»–åˆ†æï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿æŒæ¯å±‚ç‹¬ç«‹ï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬è®¡ç®—ä¸€ä¸ªæ€»ä½“çš„expert_usage
    # è¿™ä¸ªå€¼ä¸»è¦ç”¨äºå¯è§†åŒ–ï¼Œå®é™…åˆ†æåº”è¯¥ä½¿ç”¨layer_expert_usage
    combined_expert_usage = np.mean(layer_expert_usage, axis=0)  # [E] - å„å±‚å¹³å‡åçš„ä¸“å®¶ä½¿ç”¨ç‡

    print("æ•´ä½“ç»Ÿè®¡ï¼ˆå„å±‚å¹³å‡ï¼‰:")
    print(f"  æ¨¡å‹å±‚æ•°: {n_layers}")
    print(f"  æ¯å±‚ä¸“å®¶æ•°: {n_experts}")
    print(f"  æ•´ä½“å¹³è¡¡åº¦: {overall_balance:.4f} (å„å±‚å¹³è¡¡åº¦çš„å¹³å‡å€¼)")
    print(f"  å±‚é—´å¹³è¡¡åº¦å·®å¼‚: {np.std(layer_balance_scores):.4f}")
    print()

    print("è·¯ç”±å¤šæ ·æ€§ç»Ÿè®¡ï¼ˆç†µï¼‰:")
    print(f"  å„å±‚å¹³å‡ç†µ: {overall_avg_entropy:.4f}")
    print(f"  å±‚é—´ç†µæ ‡å‡†å·®: {overall_entropy_std:.4f}")
    print(f"  æ ·æœ¬å¹³å‡ç†µ: {sample_avg_entropy.mean():.4f} Â± {sample_avg_entropy.std():.4f}")
    print(f"  æœ€å¤§å¯èƒ½ç†µï¼ˆå•å±‚ï¼‰: {np.log(n_experts):.4f}")
    print()

    # è¿”å›è¯¦ç»†çš„å±‚çº§æ•°æ®ä»¥åŠå…¼å®¹æ€§æ•°æ®
    return {
        "combined_expert_usage": combined_expert_usage,  # å…¼å®¹æ€§ï¼šç”¨äºå¯è§†åŒ–
        "layer_expert_usage": layer_expert_usage,  # æ–°å¢ï¼šæ¯å±‚çš„ä¸“å®¶ä½¿ç”¨ç‡
        "layer_balance_scores": layer_balance_scores,  # æ–°å¢ï¼šæ¯å±‚çš„å¹³è¡¡åº¦
        "overall_balance": overall_balance,  # æ–°å¢ï¼šæ•´ä½“å¹³è¡¡åº¦
        "sample_avg_entropy": sample_avg_entropy,  # å…¼å®¹æ€§ï¼šæ¯ä¸ªæ ·æœ¬çš„å¹³å‡ç†µ
        "layer_entropy_stats": layer_entropy_stats,  # æ–°å¢ï¼šæ¯å±‚çš„ç†µç»Ÿè®¡
        "overall_entropy_stats": {
            "layer_avg_entropy": overall_avg_entropy,
            "layer_entropy_std": overall_entropy_std,
            "sample_avg_entropy_mean": sample_avg_entropy.mean(),
            "sample_avg_entropy_std": sample_avg_entropy.std(),
        },
    }


def perform_clustering_and_visualization(router_data, device=None, clustering_method="kmeans"):
    """
    ä½¿ç”¨GPUèšç±»ç®—æ³•è¿›è¡Œèšç±»å¹¶å‡†å¤‡2DæŠ•å½±å¯è§†åŒ–
    """
    moe_logits = router_data["moe_logits"]  # [N, L, E] - å·²ç»æ˜¯æ¦‚ç‡åˆ†å¸ƒ
    n_samples = len(moe_logits)

    # å¦‚æœæ²¡æœ‰æŒ‡å®šè®¾å¤‡ï¼Œè‡ªåŠ¨é€‰æ‹©
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 1. å‡†å¤‡èšç±»ç‰¹å¾ï¼šå°† [N, L, E] å±•å¹³ä¸º [N, L*E]
    features_matrix = moe_logits.reshape(n_samples, -1).to(device=device, dtype=torch.float32)

    # 2. ä½¿ç”¨GPUèšç±»ç®—æ³•
    cluster_selector = ClusterBasedSelection(device=device, debug_print=False)

    # å‡†å¤‡èšç±»å‚æ•°
    clustering_params = {
        "auto_k": False,
        "k_range": [max(2, min(10, n_samples // 100)), min(50, n_samples // 10)],
        "k": 26,
        "max_iters": 300,
    }

    if clustering_method.lower() == "kmeans":
        cluster_labels, cluster_info = cluster_selector._kmeans_clustering(features_matrix, clustering_params)
    elif clustering_method.lower() == "hdbscan":
        hdbscan_params = {
            "min_cluster_size": max(10, n_samples // 100),
            "metric": "cosine",
            "use_gpu": True,
        }
        cluster_labels, cluster_info = cluster_selector._hdbscan_clustering(features_matrix, hdbscan_params)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„èšç±»æ–¹æ³•: {clustering_method}")

    # 3. 2DæŠ•å½±ä½¿ç”¨GPUåŠ é€Ÿçš„t-SNE
    features_cpu = features_matrix.cpu().numpy()

    coords_2d = gpu_accelerated_tsne(features_cpu, n_components=2, perplexity=min(30, n_samples // 4), random_state=42, use_gpu=True)

    return coords_2d, cluster_labels.cpu().numpy(), cluster_info


def create_simplified_visualization(router_data, coords_2d, cluster_labels, cluster_info, quality_scores, save_path=None):
    """åˆ›å»ºç®€åŒ–çš„ä¸‰ä¸ªå­å›¾å¯è§†åŒ–"""
    sample_ids = router_data["sample_ids"]
    quality_score_raw = router_data["quality_score"]  # [N, L, 1]

    # ä½¿ç”¨1x3å¸ƒå±€
    fig, axes = plt.subplots(1, 3, figsize=(21, 6))
    plt.subplots_adjust(wspace=0.3)

    # å­å›¾1: æ ·æœ¬è´¨é‡åˆ†æ•°åˆ†å¸ƒ
    ax1 = axes[0]
    ax1.hist(quality_scores, bins=50, alpha=0.7, color="skyblue", edgecolor="black")
    ax1.set_xlabel("è´¨é‡åˆ†æ•°")
    ax1.set_ylabel("æ ·æœ¬æ•°é‡")
    ax1.set_title(f"æ ·æœ¬è´¨é‡åˆ†æ•°åˆ†å¸ƒ\næ€»æ ·æœ¬: {len(quality_scores)}ä¸ª")
    ax1.grid(True, alpha=0.3)

    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    mean_score = quality_scores.mean()
    std_score = quality_scores.std()
    ax1.axvline(mean_score, color="red", linestyle="--", alpha=0.7, label=f"å¹³å‡å€¼: {mean_score:.3f}")
    ax1.legend()

    # å­å›¾2: å„å±‚è´¨é‡åˆ†æ•°åˆ†å¸ƒ
    ax2 = axes[1]
    quality_by_layer = torch.sigmoid(quality_score_raw.float()).squeeze(-1)  # [N, L]
    layer_avg_quality = quality_by_layer.mean(dim=0).numpy()  # [L]
    layer_std_quality = quality_by_layer.std(dim=0).numpy()  # [L]

    x_layers = range(len(layer_avg_quality))
    ax2.plot(x_layers, layer_avg_quality, "o-", linewidth=2, markersize=6, color="blue", label="å¹³å‡è´¨é‡")
    ax2.fill_between(x_layers, layer_avg_quality - layer_std_quality, layer_avg_quality + layer_std_quality, alpha=0.2, color="blue", label="æ ‡å‡†å·®")
    ax2.set_xlabel("å±‚ç´¢å¼•")
    ax2.set_ylabel("å¹³å‡è´¨é‡åˆ†æ•°")
    ax2.set_title("å„å±‚è´¨é‡åˆ†æ•°åˆ†å¸ƒ")
    ax2.grid(True, alpha=0.3)
    ax2.legend()

    # å­å›¾3: äºŒçº§è·¯ç”± 2D æŠ•å½± + èšç±»ç»“æœ
    ax3 = axes[2]

    # è·å–ç‹¬ç‰¹çš„èšç±»æ ‡ç­¾
    unique_labels = np.unique(cluster_labels)
    n_clusters = len(unique_labels[unique_labels >= 0])  # æ’é™¤å™ªå£°ç‚¹(-1)

    # ä¸ºä¸åŒç°‡ç”Ÿæˆé¢œè‰²
    colors = plt.cm.Set1(np.linspace(0, 1, max(n_clusters, 1)))

    # ç»˜åˆ¶æ•°æ®ç‚¹ (å‡å°ç‚¹çš„åŠå¾„)
    for i, label in enumerate(unique_labels):
        if label == -1:
            # å™ªå£°ç‚¹ç”¨ç°è‰²æ˜¾ç¤º (s ä» 20 å‡å°åˆ° 8)
            mask = cluster_labels == label
            ax3.scatter(coords_2d[mask, 0], coords_2d[mask, 1], c="gray", s=1, alpha=0.5, label=f"å™ªå£° ({np.sum(mask)}ä¸ª)", marker="x")
        else:
            # æ­£å¸¸ç°‡ç”¨ä¸åŒé¢œè‰² (s ä» 30 å‡å°åˆ° 12)
            mask = cluster_labels == label
            color = colors[i % len(colors)] if i < len(colors) else colors[i % len(colors)]
            ax3.scatter(coords_2d[mask, 0], coords_2d[mask, 1], c=[color], s=2, alpha=0.7, label=f"ç°‡ {label} ({np.sum(mask)}ä¸ª)")

    ax3.set_xlabel("ç¬¬ä¸€ä¸»æˆåˆ†")
    ax3.set_ylabel("ç¬¬äºŒä¸»æˆåˆ†")
    ax3.set_title(f"äºŒçº§è·¯ç”± 2D æŠ•å½±ä¸èšç±»ç»“æœ\n{cluster_info.get('method', 'Unknown')} | {n_clusters}ä¸ªç°‡")
    ax3.grid(True, alpha=0.3)

    # åªåœ¨ç°‡æ•°ä¸å¤ªå¤šæ—¶æ˜¾ç¤ºå›¾ä¾‹
    if len(unique_labels) <= 10:
        ax3.legend(bbox_to_anchor=(1.05, 1), loc="upper left", fontsize="small")

    # æ„å»ºæ ‡é¢˜ä¿¡æ¯
    title_info = f"Select-MoE æ•°æ®é€‰æ‹©åˆ†æ - {router_data.get('dataset_name', 'Unknown')}"
    subtitle_info = f"æ€»æ ·æœ¬: {len(sample_ids)}, å¹³å‡è´¨é‡: {mean_score:.3f} Â± {std_score:.3f}"

    # å¦‚æœæ˜¯èšåˆæ•°æ®é›†ï¼Œæ·»åŠ æ•°æ®é›†æ„æˆä¿¡æ¯
    if "source_datasets" in router_data:
        datasets_info = ", ".join(router_data["source_datasets"])
        if len(datasets_info) > 60:  # å¦‚æœå¤ªé•¿åˆ™æˆªæ–­
            datasets_info = datasets_info[:57] + "..."
        subtitle_info += f"\nåŒ…å«æ•°æ®é›†: {datasets_info}"

    plt.suptitle(title_info + "\n" + subtitle_info, fontsize=14, y=0.98)

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
        print(f"âœ“ å·²ä¿å­˜ç®€åŒ–åˆ†æå›¾: {save_path}")

    return fig


def detect_processing_mode(router_data_path):
    """æ™ºèƒ½æ£€æµ‹å¤„ç†æ¨¡å¼å¹¶è¿”å›é…ç½®ä¿¡æ¯

    Returns:
        dict: {
            'mode': 'batch' | 'single_experiment' | 'single_file',
            'description': str,
            'auto_aggregate': bool,
            'paths': list  # éœ€è¦å¤„ç†çš„è·¯å¾„åˆ—è¡¨
        }
    """
    if os.path.isfile(router_data_path):
        if router_data_path.endswith(".pt"):
            return {"mode": "single_file", "description": "å•æ–‡ä»¶æ¨¡å¼ - åˆ†æå•ä¸ªæ•°æ®é›†æ–‡ä»¶", "auto_aggregate": False, "paths": [router_data_path]}
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {router_data_path}")

    elif os.path.isdir(router_data_path):
        # æ£€æŸ¥ç›®å½•å†…å®¹æ¥åˆ¤æ–­æ¨¡å¼
        dir_contents = os.listdir(router_data_path)

        # æ£€æŸ¥æ˜¯å¦åŒ…å« *_router_data.pt æ–‡ä»¶ï¼ˆå•å®éªŒæ¨¡å¼ï¼‰
        router_data_files = [f for f in dir_contents if f.endswith("_router_data.pt")]
        if router_data_files:
            return {
                "mode": "single_experiment",
                "description": "å•å®éªŒåˆå¹¶æ¨¡å¼ - åˆå¹¶å•ä¸ªå®éªŒå†…çš„æ‰€æœ‰æ•°æ®é›†",
                "auto_aggregate": True,
                "paths": [router_data_path],
            }

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å®éªŒå­ç›®å½•ï¼ˆæ‰¹é‡å¤„ç†æ¨¡å¼ï¼‰
        experiment_dirs = []
        for item in dir_contents:
            item_path = os.path.join(router_data_path, item)
            if os.path.isdir(item_path):
                # æ£€æŸ¥å­ç›®å½•æ˜¯å¦åŒ…å«router_dataç›®å½•
                router_data_subdir = os.path.join(item_path, "router_data")
                if os.path.isdir(router_data_subdir):
                    # æ£€æŸ¥router_dataç›®å½•æ˜¯å¦åŒ…å«.ptæ–‡ä»¶
                    router_files = glob.glob(os.path.join(router_data_subdir, "*_router_data.pt"))
                    if router_files:
                        experiment_dirs.append(router_data_subdir)

        if experiment_dirs:
            return {
                "mode": "batch",
                "description": f"æ‰¹é‡å¤„ç†æ¨¡å¼ - å¤„ç†{len(experiment_dirs)}ä¸ªå®éªŒï¼Œæ¯ä¸ªå®éªŒåˆå¹¶æ•°æ®é›†",
                "auto_aggregate": True,
                "paths": experiment_dirs,
            }

        # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œå°è¯•æŸ¥æ‰¾åµŒå¥—çš„router_dataç›®å½•
        nested_router_dirs = glob.glob(os.path.join(router_data_path, "*/router_data"))
        if nested_router_dirs:
            valid_dirs = []
            for router_dir in nested_router_dirs:
                router_files = glob.glob(os.path.join(router_dir, "*_router_data.pt"))
                if router_files:
                    valid_dirs.append(router_dir)

            if valid_dirs:
                return {
                    "mode": "batch",
                    "description": f"æ‰¹é‡å¤„ç†æ¨¡å¼ - å¤„ç†{len(valid_dirs)}ä¸ªå®éªŒï¼Œæ¯ä¸ªå®éªŒåˆå¹¶æ•°æ®é›†",
                    "auto_aggregate": True,
                    "paths": valid_dirs,
                }

        raise ValueError(f"ç›®å½• {router_data_path} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„routeræ•°æ®æ–‡ä»¶æˆ–å­å®éªŒç›®å½•")

    else:
        raise ValueError(f"è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆçš„æ–‡ä»¶/ç›®å½•: {router_data_path}")


def parse_experiment_config(experiment_path):
    """ä»å®éªŒè·¯å¾„è§£æé…ç½®ä¿¡æ¯ç”¨äºæ–‡ä»¶å‘½å

    Args:
        experiment_path: å®éªŒç›®å½•è·¯å¾„ï¼Œå¦‚ outputs/stage_2_selection/2025-08-11/03-52-40-batch=8_lr=0.001_loss=beta_moment_matching_tag=none/router_data

    Returns:
        str: æ ¼å¼åŒ–çš„é…ç½®åç§°ï¼Œå¦‚ '03-52-40-batch=8_lr=0.001_loss=beta_moment_matching_tag=none'
    """
    # è·å–å®éªŒç›®å½•åï¼ˆrouter_dataçš„çˆ¶ç›®å½•ï¼‰
    if experiment_path.endswith("router_data"):
        experiment_dir = os.path.dirname(experiment_path)
    else:
        experiment_dir = experiment_path

    # æå–ç›®å½•å
    experiment_name = os.path.basename(experiment_dir)

    # å¦‚æœç›®å½•ååŒ…å«æ—¶é—´æˆ³å’Œé…ç½®ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
    if "-" in experiment_name and ("=" in experiment_name or "batch" in experiment_name or "lr" in experiment_name):
        return experiment_name

    # å¦åˆ™ä½¿ç”¨å®Œæ•´è·¯å¾„çš„æœ€åä¸¤çº§ç›®å½•å
    parent_dir = os.path.basename(os.path.dirname(experiment_dir))
    return f"{parent_dir}_{experiment_name}"


def load_all_router_data_files(router_data_path):
    """åŠ è½½router_dataæ–‡ä»¶æˆ–ç›®å½•ä¸­çš„æ‰€æœ‰router_dataæ–‡ä»¶"""
    if os.path.isfile(router_data_path) and router_data_path.endswith(".pt"):
        # å•ä¸ªæ–‡ä»¶
        return {os.path.basename(router_data_path).replace("_router_data.pt", ""): load_router_data(router_data_path)}
    elif os.path.isdir(router_data_path):
        # ç›®å½•ï¼ŒæŸ¥æ‰¾æ‰€æœ‰_router_data.ptæ–‡ä»¶
        router_data_files = glob.glob(os.path.join(router_data_path, "*_router_data.pt"))
        if not router_data_files:
            raise ValueError(f"åœ¨ç›®å½• {router_data_path} ä¸­æœªæ‰¾åˆ°ä»»ä½•_router_data.ptæ–‡ä»¶")

        all_router_data = {}
        for file_path in sorted(router_data_files):
            dataset_name = os.path.basename(file_path).replace("_router_data.pt", "")
            all_router_data[dataset_name] = load_router_data(file_path)

        return all_router_data
    else:
        raise ValueError(f"è·¯å¾„ä¸æ˜¯æœ‰æ•ˆçš„.ptæ–‡ä»¶æˆ–ç›®å½•: {router_data_path}")


def batch_process_experiments(experiment_paths, args):
    """æ‰¹é‡å¤„ç†å¤šä¸ªå®éªŒçš„routeræ•°æ®å¹¶ç”Ÿæˆåˆ†ææŠ¥å‘Š

    Args:
        experiment_paths: list of experiment router_data directory paths
        args: command line arguments

    Returns:
        list: æ‰€æœ‰å®éªŒçš„åˆ†æç»“æœ
    """
    all_experiment_results = []

    experiment_output_dir = os.path.join(args.output_dir, "batch_analysis")
    os.makedirs(experiment_output_dir, exist_ok=True)
    args.output_dir = experiment_output_dir

    for experiment_path in experiment_paths:
        try:
            # è§£æå®éªŒé…ç½®åç§°
            config_name = parse_experiment_config(experiment_path)

            # åŠ è½½è¯¥å®éªŒçš„æ‰€æœ‰routeræ•°æ®
            experiment_router_data = load_all_router_data_files(experiment_path)

            if len(experiment_router_data) == 0:
                continue

            # å¦‚æœæœ‰å¤šä¸ªæ•°æ®é›†ï¼Œè‡ªåŠ¨èšåˆ
            if len(experiment_router_data) > 1:
                aggregated_data = aggregate_router_data(experiment_router_data)
            else:
                # åªæœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œç›´æ¥ä½¿ç”¨
                dataset_name, router_data = next(iter(experiment_router_data.items()))
                aggregated_data = router_data
                aggregated_data["dataset_name"] = f"{config_name} - {dataset_name}"

            # åˆ†æè¯¥å®éªŒ
            result = analyze_single_dataset(config_name, aggregated_data, args)
            result["config_name"] = config_name
            result["experiment_path"] = experiment_path
            all_experiment_results.append(result)

        except Exception:
            continue

    # ç”Ÿæˆæ‰¹é‡å¤„ç†æ±‡æ€»æŠ¥å‘Š
    if all_experiment_results:
        generate_batch_analysis_report(all_experiment_results, args)

    return all_experiment_results


def generate_batch_analysis_report(all_results, args):
    """ç”Ÿæˆæ‰¹é‡åˆ†æçš„æ±‡æ€»æŠ¥å‘Š"""
    print(f"\n\n{'=' * 90}")
    print("ğŸ“ˆ æ‰¹é‡åˆ†ææ±‡æ€»æŠ¥å‘Š")
    print("=" * 90)

    successful_experiments = len(all_results)
    total_samples = sum(r["n_total"] for r in all_results)

    print(f"æˆåŠŸåˆ†æäº† {successful_experiments} ä¸ªå®éªŒï¼Œå…± {total_samples:,} ä¸ªæ ·æœ¬")
    print()

    # ç»Ÿè®¡ä¿¡æ¯è¡¨
    print("å„å®éªŒè¯¦ç»†ç»Ÿè®¡:")
    print(f"{'=' * 100}")
    print(f"{'Config Name':<50} {'Samples':<8} {'Quality':<10} {'Entropy':<10} {'Clusters':<8} {'Balance':<10}")
    print("-" * 100)

    quality_scores = []
    entropy_scores = []
    cluster_counts = []
    balance_scores = []

    for result in all_results:
        avg_quality = result["quality_scores"].mean()
        avg_entropy = result["sample_entropy"].mean()
        # ä½¿ç”¨æ–°çš„overall_balanceå€¼
        balance = result.get("overall_balance", 0.0)
        if balance == 0.0 and "expert_usage" in result:
            # å…¼å®¹æ—§ç‰ˆæœ¬çš„è®¡ç®—æ–¹å¼
            expert_usage_array = result["expert_usage"]
            if isinstance(expert_usage_array, np.ndarray):
                balance = 1 - expert_usage_array.std() * len(expert_usage_array)
            else:
                balance = 0.0

        quality_scores.append(avg_quality)
        entropy_scores.append(avg_entropy)
        cluster_counts.append(result["n_clusters"])
        balance_scores.append(balance)

        # æˆªæ–­é…ç½®åç§°ä»¥é€‚åˆæ˜¾ç¤º
        config_display = result["config_name"][:47] + "..." if len(result["config_name"]) > 50 else result["config_name"]

        print(f"{config_display:<50} {result['n_total']:<8} {avg_quality:<10.4f} {avg_entropy:<10.4f} {result['n_clusters']:<8} {balance:<10.4f}")

    print("\n")

    # æ±‡æ€»ç»Ÿè®¡
    print("æ±‡æ€»ç»Ÿè®¡:")
    print(f"Quality Score - Min: {min(quality_scores):.4f}, Max: {max(quality_scores):.4f}, Avg: {sum(quality_scores) / len(quality_scores):.4f}")
    print(f"Routing Entropy - Min: {min(entropy_scores):.4f}, Max: {max(entropy_scores):.4f}, Avg: {sum(entropy_scores) / len(entropy_scores):.4f}")
    print(f"Cluster Count - Min: {min(cluster_counts)}, Max: {max(cluster_counts)}, Avg: {sum(cluster_counts) / len(cluster_counts):.1f}")
    print(f"Expert Balance - Min: {min(balance_scores):.4f}, Max: {max(balance_scores):.4f}, Avg: {sum(balance_scores) / len(balance_scores):.4f}")

    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    report_path = os.path.join(args.output_dir, "batch_analysis_report.txt")
    os.makedirs(args.output_dir, exist_ok=True)

    with open(report_path, "w", encoding="utf-8") as f:
        f.write("Select-MoE æ‰¹é‡åˆ†ææŠ¥å‘Š\n")
        f.write(f"{'=' * 50}\n")
        f.write(f"åˆ†ææ—¶é—´: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æˆåŠŸåˆ†æ: {successful_experiments} ä¸ªå®éªŒ\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {total_samples:,} ä¸ª\n\n")

        f.write("å„å®éªŒè¯¦æƒ…:\n")
        f.write("-" * 120 + "\n")
        for result in all_results:
            avg_quality = result["quality_scores"].mean()
            avg_entropy = result["sample_entropy"].mean()
            # ä½¿ç”¨æ–°çš„overall_balanceå€¼
            balance = result.get("overall_balance", 0.0)
            if balance == 0.0 and "expert_usage" in result:
                # å…¼å®¹æ—§ç‰ˆæœ¬çš„è®¡ç®—æ–¹å¼
                expert_usage_array = result["expert_usage"]
                if isinstance(expert_usage_array, np.ndarray):
                    balance = 1 - expert_usage_array.std() * len(expert_usage_array)
                else:
                    balance = 0.0

            f.write(f"Config: {result['config_name']}\n")
            f.write(f"  Path: {result['experiment_path']}\n")
            f.write(f"  Samples: {result['n_total']:,}, Quality: {avg_quality:.4f}, Entropy: {avg_entropy:.4f}\n")
            f.write(f"  Clusters: {result['n_clusters']}, Balance: {balance:.4f}\n\n")

    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
    print(f"ğŸ–¼ï¸  æ‰€æœ‰å®éªŒçš„åˆ†æå›¾è¡¨å·²ä¿å­˜åœ¨: {os.path.join(args.output_dir, 'batch_analysis')}")
    print("=" * 90)


def aggregate_router_data(all_router_data):
    """å°†å¤šä¸ªæ•°æ®é›†çš„router_dataåˆå¹¶ä¸ºä¸€ä¸ªæ•´ä½“æ•°æ®é›†"""
    if len(all_router_data) == 1:
        # åªæœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œç›´æ¥è¿”å›
        dataset_name, router_data = next(iter(all_router_data.items()))
        router_data["dataset_name"] = f"{dataset_name} (å•ä¸€æ•°æ®é›†)"
        return router_data

    # æ”¶é›†æ‰€æœ‰æ•°æ®
    all_quality_logits = []
    all_moe_logits = []
    all_sample_ids = []

    dataset_names = list(all_router_data.keys())
    sample_counts = []

    # éªŒè¯å¼ é‡å½¢çŠ¶å…¼å®¹æ€§
    reference_quality_shape = None
    reference_moe_shape = None

    for dataset_name, router_data in all_router_data.items():
        quality_score = router_data["quality_score"]  # [N, L, 1] - æ–°æ ¼å¼
        moe_logits = router_data["moe_logits"]  # [N, L, E]

        # æ£€æŸ¥å½¢çŠ¶å…¼å®¹æ€§
        if reference_quality_shape is None:
            reference_quality_shape = quality_score.shape[1:]  # [L, 1]
            reference_moe_shape = moe_logits.shape[1:]  # [L, E]
        else:
            if quality_score.shape[1:] != reference_quality_shape:
                raise ValueError(f"æ•°æ®é›† {dataset_name} çš„quality_scoreå½¢çŠ¶ {quality_score.shape[1:]} ä¸å‚è€ƒå½¢çŠ¶ {reference_quality_shape} ä¸å…¼å®¹")
            if moe_logits.shape[1:] != reference_moe_shape:
                raise ValueError(f"æ•°æ®é›† {dataset_name} çš„moe_logitså½¢çŠ¶ {moe_logits.shape[1:]} ä¸å‚è€ƒå½¢çŠ¶ {reference_moe_shape} ä¸å…¼å®¹")

        sample_count = len(router_data["sample_ids"])
        sample_counts.append(sample_count)

        # æ·»åŠ æ•°æ®é›†å‰ç¼€åˆ°sample_idsä»¥é¿å…å†²çª
        prefixed_sample_ids = [f"{dataset_name}_{sid}" for sid in router_data["sample_ids"]]

        all_quality_logits.append(quality_score)
        all_moe_logits.append(moe_logits)
        all_sample_ids.extend(prefixed_sample_ids)

    # åˆå¹¶å¼ é‡
    aggregated_quality_logits = torch.cat(all_quality_logits, dim=0)
    aggregated_moe_logits = torch.cat(all_moe_logits, dim=0)

    # æ„å»ºèšåˆåçš„router_data
    aggregated_data = {
        "quality_score": aggregated_quality_logits,  # ä½¿ç”¨æ–°çš„é”®å
        "moe_logits": aggregated_moe_logits,
        "sample_ids": all_sample_ids,
        "dataset_name": f"æ‰€æœ‰æ•°æ®é›†èšåˆ ({len(dataset_names)}ä¸ªæ•°æ®é›†)",
        "source_datasets": dataset_names,
        "dataset_sample_counts": dict(zip(dataset_names, sample_counts, strict=True)),
    }

    return aggregated_data


def analyze_single_dataset(dataset_name, router_data, args, config_name=None):
    """åˆ†æå•ä¸ªæ•°æ®é›†"""
    # 1. åˆ†æè´¨é‡é—¨
    quality_scores, _ = analyze_quality_gates(router_data)

    # 2. åˆ†æMoEè·¯ç”±
    routing_analysis = analyze_moe_routing(router_data)
    expert_usage = routing_analysis["combined_expert_usage"]  # ç”¨äºå…¼å®¹æ€§
    sample_entropy = routing_analysis["sample_avg_entropy"]
    overall_balance = routing_analysis["overall_balance"]

    # 3. æ‰§è¡Œèšç±»å’Œ2DæŠ•å½±
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    coords_2d, cluster_labels, cluster_info = perform_clustering_and_visualization(router_data, device=device, clustering_method=args.clustering_method)

    # 4. åˆ›å»ºç®€åŒ–å¯è§†åŒ–
    save_path = None

    # æ ¹æ®é…ç½®åç§°æˆ–æ•°æ®é›†åç§°ç”Ÿæˆä¿å­˜è·¯å¾„
    if config_name and config_name != dataset_name:
        # æ‰¹é‡å¤„ç†æ¨¡å¼ï¼Œä½¿ç”¨é…ç½®åç§°
        safe_name = config_name.replace("/", "_").replace("\\", "_")
        save_path = os.path.join(args.output_dir, f"{safe_name}.png")
    else:
        # å•æ•°æ®é›†åˆ†ææ¨¡å¼
        if dataset_name == "èšåˆæ•°æ®é›†" and "source_datasets" in router_data:
            # èšåˆæ•°æ®é›†ä½¿ç”¨ç‰¹æ®Šå‘½å
            safe_dataset_name = f"aggregated_{'_'.join(router_data['source_datasets'])}"
            safe_dataset_name = safe_dataset_name.replace("/", "_").replace("\\", "_")
        else:
            safe_dataset_name = dataset_name.replace("/", "_").replace("\\", "_")
        save_path = os.path.join(args.output_dir, f"{safe_dataset_name}.png")

    create_simplified_visualization(router_data, coords_2d, cluster_labels, cluster_info, quality_scores, save_path)

    plt.show()

    # 5. ç”Ÿæˆåˆ†ææŠ¥å‘Š
    n_total = len(router_data["sample_ids"])
    n_clusters = len(np.unique(cluster_labels[cluster_labels >= 0]))

    return {
        "dataset_name": dataset_name,
        "quality_scores": quality_scores,
        "expert_usage": expert_usage,
        "sample_entropy": sample_entropy,
        "cluster_info": cluster_info,
        "n_total": n_total,
        "n_clusters": n_clusters,
        "overall_balance": overall_balance,  # æ–°å¢ï¼šæ•´ä½“å¹³è¡¡åº¦
        "routing_analysis": routing_analysis,  # æ–°å¢ï¼šè¯¦ç»†çš„è·¯ç”±åˆ†æç»“æœ
    }


def main():
    parser = argparse.ArgumentParser(description="Select-MoEæ™ºèƒ½æ•°æ®é€‰æ‹©åˆ†æ")
    parser.add_argument("router_data_path", help="è·¯ç”±æ•°æ®æ–‡ä»¶è·¯å¾„(.ptæ ¼å¼)æˆ–åŒ…å«å¤šä¸ªrouter_dataæ–‡ä»¶çš„ç›®å½•")
    parser.add_argument("--clustering-method", choices=["kmeans", "hdbscan"], default="kmeans", help="èšç±»æ–¹æ³• (é»˜è®¤: kmeans)")
    parser.add_argument("--output-dir", default="./outputs/visual_figs", help="å›¾ç‰‡ä¿å­˜ç›®å½•")
    parser.add_argument("--dataset-filter", help="åªåˆ†æåŒ¹é…æ­¤æ¨¡å¼çš„æ•°æ®é›† (æ”¯æŒé€šé…ç¬¦)")
    parser.add_argument("--aggregate-datasets", action="store_true", help="å°†å¤šä¸ªæ•°æ®é›†èšåˆä¸ºä¸€ä¸ªæ•´ä½“è¿›è¡Œåˆ†æï¼Œè€Œä¸æ˜¯åˆ†åˆ«åˆ†ææ¯ä¸ªæ•°æ®é›†")

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)

    print("Select-MoEæ™ºèƒ½æ•°æ®é€‰æ‹©ç»¼åˆåˆ†æ")
    print("=" * 70)

    # ğŸ” æ™ºèƒ½æ£€æµ‹å¤„ç†æ¨¡å¼
    print(f"ğŸ” åˆ†æè·¯å¾„: {args.router_data_path}")

    try:
        mode_config = detect_processing_mode(args.router_data_path)
        print(f"âœ… æ£€æµ‹åˆ°å¤„ç†æ¨¡å¼: {mode_config['mode']}")
        print(f"ğŸ“„ {mode_config['description']}")
        print(f"ğŸ¯ å¾…å¤„ç†è·¯å¾„æ•°é‡: {len(mode_config['paths'])}")
        print()

        # æ ¹æ®æ¨¡å¼è‡ªåŠ¨è®¾ç½®èšåˆé€‰é¡¹
        if mode_config["auto_aggregate"]:
            args.aggregate_datasets = True
            print("ğŸ”§ è‡ªåŠ¨å¯ç”¨æ•°æ®é›†èšåˆæ¨¡å¼")

    except ValueError as e:
        print(f"âŒ è·¯å¾„æ£€æµ‹å¤±è´¥: {e}")
        return

    # ğŸš€ æ ¹æ®æ£€æµ‹åˆ°çš„æ¨¡å¼æ‰§è¡Œç›¸åº”çš„å¤„ç†é€»è¾‘
    if mode_config["mode"] == "batch":
        # æ‰¹é‡å¤„ç†æ¨¡å¼ï¼šå¤„ç†å¤šä¸ªå®éªŒç›®å½•
        print("\nğŸ”„ å¯åŠ¨æ‰¹é‡å¤„ç†æ¨¡å¼")
        batch_process_experiments(mode_config["paths"], args)

    elif mode_config["mode"] == "single_experiment":
        # å•å®éªŒæ¨¡å¼ï¼šå¤„ç†å•ä¸ªå®éªŒç›®å½•å†…çš„router_data
        print("\nâš™ï¸ å¯åŠ¨å•å®éªŒåˆ†ææ¨¡å¼")

        router_data_path = mode_config["paths"][0]
        print(f"åŠ è½½è·¯ç”±æ•°æ®: {router_data_path}")
        all_router_data = load_all_router_data_files(router_data_path)

        # è¿‡æ»¤æ•°æ®é›†
        if args.dataset_filter:
            import fnmatch

            filtered_data = {}
            for dataset_name in all_router_data:
                if fnmatch.fnmatch(dataset_name, args.dataset_filter):
                    filtered_data[dataset_name] = all_router_data[dataset_name]
            all_router_data = filtered_data
            print(f"åº”ç”¨è¿‡æ»¤å™¨ '{args.dataset_filter}', åŒ¹é…åˆ° {len(all_router_data)} ä¸ªæ•°æ®é›†")

        print(f"å°†åˆ†æ {len(all_router_data)} ä¸ªæ•°æ®é›†: {list(all_router_data.keys())}")

        all_results = []

        if args.aggregate_datasets and len(all_router_data) > 1:
            # èšåˆæ¨¡å¼ï¼šåˆå¹¶æ‰€æœ‰æ•°æ®é›†åç»Ÿä¸€åˆ†æ
            print("\nå¯ç”¨èšåˆåˆ†ææ¨¡å¼ - å°†æ‰€æœ‰æ•°æ®é›†åˆå¹¶ä¸ºä¸€ä¸ªæ•´ä½“")
            aggregated_data = aggregate_router_data(all_router_data)
            result = analyze_single_dataset("èšåˆæ•°æ®é›†", aggregated_data, args)
            all_results.append(result)

            # ä¸ºèšåˆæ¨¡å¼æ·»åŠ é¢å¤–çš„æ•°æ®é›†æ„æˆä¿¡æ¯
            print(f"\n{'=' * 70}")
            print("èšåˆæ•°æ®é›†æ„æˆè¯¦æƒ…")
            print("=" * 70)
            for dataset_name, count in aggregated_data["dataset_sample_counts"].items():
                percentage = count / len(aggregated_data["sample_ids"]) * 100
                print(f"  {dataset_name}: {count} æ ·æœ¬ ({percentage:.1f}%)")
            print()

        else:
            # åŸæœ‰æ¨¡å¼ï¼šåˆ†åˆ«åˆ†ææ¯ä¸ªæ•°æ®é›†
            if args.aggregate_datasets:
                print("åªæœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œèšåˆæ¨¡å¼æ— æ•ˆï¼Œä½¿ç”¨æ ‡å‡†åˆ†ææ¨¡å¼")

            for dataset_name, router_data in all_router_data.items():
                result = analyze_single_dataset(dataset_name, router_data, args)
                all_results.append(result)

        # ç”Ÿæˆæ€»ä½“åˆ†ææŠ¥å‘Š (ä»…åœ¨éèšåˆæ¨¡å¼ä¸‹æœ‰å¤šä¸ªæ•°æ®é›†æ—¶)
        if not args.aggregate_datasets and len(all_results) > 1:
            print(f"\n{'=' * 80}")
            print("æ€»ä½“åˆ†ææŠ¥å‘Š")
            print("=" * 80)

            total_samples = sum(r["n_total"] for r in all_results)
            avg_quality_scores = [r["quality_scores"].mean() for r in all_results]
            avg_entropy_scores = [r["sample_entropy"].mean() for r in all_results]

            print(f"åˆ†æäº† {len(all_results)} ä¸ªæ•°æ®é›†ï¼Œå…± {total_samples} ä¸ªæ ·æœ¬")
            print(f"å¹³å‡è´¨é‡åˆ†æ•°èŒƒå›´: {min(avg_quality_scores):.4f} - {max(avg_quality_scores):.4f}")
            print(f"å¹³å‡è·¯ç”±ç†µèŒƒå›´: {min(avg_entropy_scores):.4f} - {max(avg_entropy_scores):.4f}")
            print()

            # æŒ‰æ•°æ®é›†å±•ç¤ºç»Ÿè®¡ä¿¡æ¯
            print("å„æ•°æ®é›†ç»Ÿè®¡:")
            print(f"{'æ•°æ®é›†':<15} {'æ ·æœ¬æ•°':<8} {'å¹³å‡è´¨é‡':<10} {'å¹³å‡ç†µ':<10} {'ä¸“å®¶å¹³è¡¡åº¦':<12}")
            print("-" * 65)
            for result in all_results:
                balance = result.get("overall_balance", 0.0)
                if balance == 0.0 and "expert_usage" in result:
                    # å…¼å®¹æ—§ç‰ˆæœ¬çš„è®¡ç®—æ–¹å¼
                    expert_usage_array = result["expert_usage"]
                    if isinstance(expert_usage_array, np.ndarray):
                        balance = 1 - expert_usage_array.std() * len(expert_usage_array)
                    else:
                        balance = 0.0
                print(
                    f"{result['dataset_name']:<15} {result['n_total']:<8} {result['quality_scores'].mean():<10.4f} "
                    f"{result['sample_entropy'].mean():<10.4f} {balance:<12.4f}"
                )

    elif mode_config["mode"] == "single_file":
        # å•æ–‡ä»¶æ¨¡å¼ï¼šç›´æ¥åˆ†æå•ä¸ª.ptæ–‡ä»¶
        print("\nğŸ“ å¯åŠ¨å•æ–‡ä»¶åˆ†ææ¨¡å¼")

        router_data_path = mode_config["paths"][0]
        print(f"åŠ è½½è·¯ç”±æ•°æ®æ–‡ä»¶: {router_data_path}")
        all_router_data = load_all_router_data_files(router_data_path)

        # å•æ–‡ä»¶æ¨¡å¼åªæœ‰ä¸€ä¸ªæ•°æ®é›†
        dataset_name, router_data = next(iter(all_router_data.items()))
        analyze_single_dataset(dataset_name, router_data, args)

    else:
        print(f"âŒ ä¸æ”¯æŒçš„å¤„ç†æ¨¡å¼: {mode_config['mode']}")
        return

    print("=" * 70)
    print("ğŸ‰ Select-MoEæ™ºèƒ½åˆ†æå®Œæˆï¼")
    print()
    print("ğŸ“Š å…³é”®æ´å¯Ÿ:")
    print("1. è´¨é‡åˆ†æ•°åæ˜ äº†æ ·æœ¬çš„æ•´ä½“æ•°æ®è´¨é‡")
    print("2. å„å±‚è´¨é‡åˆ†æ•°åˆ†å¸ƒå±•ç¤ºäº†æ¨¡å‹å±‚çº§é—´çš„è´¨é‡åˆ¤æ–­ä¸€è‡´æ€§")
    print("3. èšç±»ç»“æœæ˜¾ç¤ºäº†MoEè·¯ç”±æ¨¡å¼çš„æ•°æ®åˆ†ç»„ç‰¹å¾")
    print("4. GPUåŠ é€Ÿçš„èšç±»ç®—æ³•æœ‰æ•ˆå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†")

    if mode_config["mode"] == "batch":
        print("5. æ‰¹é‡å¤„ç†æ¨¡å¼æ”¯æŒå¤šå®éªŒå¯¹æ¯”åˆ†æ")
        print(f"ğŸ“ æ‰¹é‡åˆ†æç»“æœä¿å­˜åœ¨: {os.path.join(args.output_dir)}")

    print("=" * 70)


if __name__ == "__main__":
    main()
