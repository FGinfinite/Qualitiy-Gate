# FSDP Configuration for Finetune Stage
# Optimized for LoRA fine-tuning with enhanced FSDP parameters matching LESS project

compute_environment: LOCAL_MACHINE
distributed_type: 'FSDP'
downcast_bf16: 'no'
fsdp_config:
  fsdp_auto_wrap_policy: 'TRANSFORMER_BASED_WRAP'
  fsdp_backward_prefetch: 'BACKWARD_PRE'  # 匹配LESS的backward_pre设置
  fsdp_cpu_ram_efficient_loading: true
  fsdp_forward_prefetch: false
  fsdp_offload_params: false
  fsdp_sharding_strategy: 'FULL_SHARD'
  fsdp_state_dict_type: 'FULL_STATE_DICT'
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: 'LlamaDecoderLayer'  # 默认为Llama，运行时会根据模型类型调整
  fsdp_use_orig_params: true  # 匹配LESS的use_orig_params设置
  limit_all_gathers: true     # 匹配LESS的limit_all_gathers设置
gpu_ids: 'all'
machine_rank: 0
main_training_function: main
mixed_precision: 'bf16'
num_machines: 1
num_processes: 1 # This will be overridden by --num_processes in the launch command
rdzv_backend: 'static'
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false
