# @package _global_

stage: pretrain
# Stage 1: Pre-training Configuration

# Hydra Configuration
hydra:
  run:
    dir: outputs/stage_1_pretrain/${now:%Y-%m-%d}/${now:%H-%M-%S}-batch=${training.batch_size}_lr=${training.learning_rate}_loss=${training.quality_loss_type}_lossWeight=${training.quality_loss_weight}_sampleWise=${training.quality_loss_params.sample_wise_averaging}_tag=${tag}
  sweep:
    dir: outputs/stage_1_pretrain
    subdir: ${now:%Y-%m-%d}/${now:%H-%M-%S}-batch=${training.batch_size}_lr=${training.learning_rate}_loss=${training.quality_loss_type}_lossWeight=${training.quality_loss_weight}_sampleWise=${training.quality_loss_params.sample_wise_averaging}_tag=${tag}
  job:
    config:
      override_dirname:
        kv_sep: "="
        item_sep: ","
        exclude_keys:
          - hydra.run.dir
          - hydra.sweep.dir
          - hydra.sweep.subdir

tag: "none"

# Model configuration
selector_model:
  path: "converted_models/select_moe_converted_OLMoE-1B-7B-0125"
  tokenizer_name: "allenai/OLMoE-1B-7B-0125"
output_dir: ${hydra:run.dir}

# Dataset configuration
dataset:
  data_dir: "dataset/train/processed"
  dataset_names:
    - "cot"
    - "dolly"
    - "flan_v2"
    - "oasst1"
  subset_ratio: 0.05
  seed: 42
  shuffle: true
  max_sequence_length: 2048
  processing_num_workers: 10
  overwrite_cache: false

# Training configuration
training:
  batch_size: 16  # 总批次大小，会根据设备数自动分配
  per_device_batch_size: 1  # 每设备批次大小，可配置
  learning_rate: 3e-4
  epochs: 4
  optimizer: "AdamW"
  scheduler: "linear"
  peft_mode: "full_rank"  # Options: "lora" (低秩微调), "full_rank" (路由参数全秩微调)
  lora:
    r: 16
    lora_alpha: 64
    lora_dropout: 0
    target_modules:
      - "mlp.gate"
  gpu_grab:
    grab: false
    memory_need_gb: 24
    over_grab: false
  # New two-tier routing architecture parameters
  quality_loss_weight: 1
  quality_gate_init_mean: 0.0
  quality_gate_init_std: 0.02
  trash_expert_mode: "zero"  # Options: "zero", "noise", "custom"
  enable_load_balancing: false
  output_router_logits: true
  # Quality loss configuration
  quality_loss_type: "sigmoid"  # Options: "sigmoid", "beta_moment_matching", "mean_variance_regularization", "mse"
  quality_loss_debug: false  # Enable debug output for quality loss computation
  quality_loss_params:
    # Beta moment matching parameters (方案一)
    beta_target_mean: 0.5
    beta_target_var: 0.05
    w_mean: 1.0
    w_var: 1.0
    # Mean-variance regularization parameters (方案二)
    lambda_var: 0.1
    # Loss averaging strategy
    sample_wise_averaging: false  # false: token-wise averaging (current), true: sample-wise averaging (avoid long sequence bias)

