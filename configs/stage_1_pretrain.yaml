# @package _global_

stage: pretrain
# Stage 1: Pre-training Configuration

# Hydra settings
hydra:
  run:
    dir: outputs/stage_1_pretrain/${now:%Y-%m-%d}/${now:%H-%M-%S}

# Model configuration
selector_model:
  name: "allenai/OLMoE-1B-7B-0125"
output_dir: ${hydra:run.dir}

# Dataset configuration
dataset:
  paths:
    - "teknium/OpenHermes-2.5"
    - "allenai/WildChat-1M"
  subset_ratio: 0.01
  seed: 42
  max_sequence_length: 512

# Training configuration
training:
  batch_size: 2
  learning_rate: 1e-4
  epochs: 4
  optimizer: "AdamW"
  scheduler: "linear"
  constraint_loss_weight: 0.2
  trash_can_loss_beta: 5.0
  trash_can_init_mean: 0.0
  trash_can_init_std: 0.02
  peft_mode: "lora"
  lora:
    r: 8
    lora_alpha: 16
    lora_dropout: 0.1
    target_modules:
      - "gate"