# @package _global_

stage: finetune
# Stage 3: Fine-tuning Configuration

# Hydra Configuration
hydra:
  run:
    dir: outputs/stage_3_finetune/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: outputs/stage_3_finetune
    subdir: ${now:%Y-%m-%d}/${now:%H-%M-%S}-${hydra.job.override_dirname}
  job:
    config:
      override_dirname:
        kv_sep: "="
        item_sep: ","
        exclude_keys:
          - hydra.run.dir
          - hydra.sweep.dir
          - hydra.sweep.subdir

# Model configuration
model:
  name: "meta-llama/Llama-2-7b-hf"

output_dir: ${hydra:run.dir}

# Dataset configuration - 使用选择后的数据
dataset:
  data_path: "outputs/stage_2_selection/2025-07-17/04-49-54/selected_data.jsonl"
  max_sequence_length: 512
  processing_num_workers: 10
  overwrite_cache: false

# Training configuration
training:
  seed: 42
  batch_size: 128  # 总批次大小，会根据设备数自动分配
  learning_rate: 2e-5
  epochs: 4
  optimizer: "AdamW"
  scheduler: "cosine"  # 余弦衰减
  warmup_ratio: 0.03    # 线性预热比例（LESS未指出）
  peft_mode: "lora"    # 使用LoRA微调
  lora:
    r: 128
    lora_alpha: 512
    lora_dropout: 0.1
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
  gpu_grab:
    grab: false
    memory_need_gb: 24
    over_grab: false