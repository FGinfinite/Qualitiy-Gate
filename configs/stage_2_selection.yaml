# @package _global_

stage: selection

# Global seed for reproducibility
seed: 42

# 模型配置
selector_model:
  # 预转换的质量门控模型路径（使用 scripts/convert_qwen_to_quality_gate.py 转换）
  # 这个路径必须与 Stage 1 使用的模型路径一致
  path: "converted_models/quality_gate_Qwen3-1.7B"
  tokenizer_name: "Qwen/Qwen3-1.7B"

# Stage 2: Data Selection Configuration (统计收集阶段)

# Hydra settings to create a timestamped output directory
hydra:
  run:
    dir: outputs/stage_2_selection/${now:%Y-%m-%d}/${now:%H-%M-%S}-${extract_config:${model_checkpoint_path}}
  sweep:
    dir: outputs/stage_2_selection
    subdir: ${now:%Y-%m-%d}/${now:%H-%M-%S}-${extract_config:${model_checkpoint_path}}
  job:
    config:
      override_dirname:
        kv_sep: "="
        item_sep: ","
        exclude_keys:
          - hydra.run.dir
          - hydra.sweep.dir
          - hydra.sweep.subdir

# Path to the stage 1 checkpoint directory
# 该目录包含训练好的质量门控权重: full_rank_weights.pt
# Stage 2 会加载 selector_model.path 的基础模型，然后应用这里的权重
model_checkpoint_path: "outputs/stage_1_warmup/2025-01-01/00-00-00-batch=16_lr=0.001_loss=sigmoid_lossWeight=1_sampleWise=True_tag=none"

# The final output path for the router data
output_path: "${hydra:run.dir}/router_data.pt"

tag: "none"

# Dataset configuration
dataset:
  # Data source selection: "local" or "hf" (should match stage 1 configuration)
  dataset_from: "hf"  # Options: "local" (本地数据集), "hf" (HuggingFace数据集)

  # Local dataset configuration (used when dataset_from="local")
  local:
    data_dir: "dataset/train/processed"
    dataset_names:
      - "cot"
      - "dolly"
      - "flan_v2"
      - "oasst1"

  # HuggingFace dataset configuration (used when dataset_from="hf")
  hf:
    datasets:
      - name: "teknium/OpenHermes-2.5"
        dataset_name: "openhermes"
        subset: null
        split: "train"

  # Common parameters
  shuffle: false
  max_sequence_length: 1024
  # 1.0 means using the full dataset
  subset_ratio: 1.0
  processing_num_workers: 10
  overwrite_cache: false

  # Sequence length sorting configuration
  sort_by_length: true  # 是否按字符串长度降序排列数据（推荐开启以优化推理效率）

# Data processing settings
data_process:
  batch_size: 32
