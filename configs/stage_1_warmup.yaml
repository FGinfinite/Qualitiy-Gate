# @package _global_

stage: warmup
# Stage 1: Quality Gate Warmup Configuration

# Hydra Configuration
defaults:
  - _self_

# Define common directory template
_target_dir_template_: ${now:%Y-%m-%d}/${now:%H-%M-%S}-batch=${training.batch_size}_lr=${training.learning_rate}_loss=${training.quality_loss_type}_lossWeight=${training.quality_loss_weight}_sampleWise=${training.quality_loss_params.sample_wise_averaging}_tag=${tag}

hydra:
  run:
    dir: outputs/stage_1_warmup/${_target_dir_template_}
  sweep:
    dir: outputs/stage_1_warmup
    subdir: ${_target_dir_template_}
  job:
    config:
      override_dirname:
        kv_sep: "="
        item_sep: ","
        exclude_keys:
          - hydra.run.dir
          - hydra.sweep.dir
          - hydra.sweep.subdir

tag: "none"

# Global seed for reproducibility  
seed: 42

# Model configuration
selector_model:
  # 必须指向预先转换好的质量门控模型路径
  # 使用 scripts/convert_qwen_to_quality_gate.py 预先完成模型转换
  path: "converted_models/quality_gate_Qwen3-1.7B-Base"
  tokenizer_name: "Qwen/Qwen3-1.7B-Base"  # 分词器路径（通常是原始Qwen3模型）

output_dir: ${hydra:run.dir}

# Dataset configuration
dataset:
  # 本地数据集根目录（所有 local 数据集的根路径）
  local_dataset_dir: "dataset/train/processed"
  
  # 数据集列表（支持混合 local 和 hf 数据集）
  datasets:
    # GSM8K 数据集 (7,473 个训练样本)
    - dataset_from: "hf"                    # 数据源: "local" 或 "hf"
      name: "openai/gsm8k"                  # HuggingFace 数据集路径
      dataset_name: "gsm8k"                 # 数据集名称（用于标识）
      subset: "main"                        # 子集名称（可选）
      split: "train"                        # 数据集分割
      format_type: "gsm8k"                  # 数据格式类型
      use_shared_memory: false              # 是否使用共享内存（可选）
    
    # HENDRYCKS_MATH 数据集 - 使用 __full_subset__ 自动加载所有 7 个子集 (7,500 个训练样本)
    # 子集包括: algebra, counting_and_probability, geometry, intermediate_algebra, 
    #          number_theory, prealgebra, precalculus
    - dataset_from: "hf"
      name: "EleutherAI/hendrycks_math"
      dataset_name: "hendrycks_math"
      subset: "__full_subset__"             # 特殊值：加载所有子集
      split: "train"
      format_type: "hendrycks_math"
      use_shared_memory: false
    
    # 可选：混合本地数据集示例
    # - dataset_from: "local"
    #   dataset_name: "oasst1"              # 对应 local_dataset_dir/oasst1 目录
    #   format_type: "standard"             # 标准格式（已包含 messages 字段）
    #   use_shared_memory: false
  
  # Common parameters
  subset_ratio: 0.025
  shuffle: true
  max_sequence_length: 1024
  processing_num_workers: 10
  overwrite_cache: false

# Training configuration
training:
  batch_size: 16  # 总批次大小，会根据设备数自动分配
  per_device_batch_size: 1  # 每设备批次大小，可配置
  learning_rate: 3e-4
  epochs: 2
  optimizer: "AdamW"
  scheduler: "linear"
  peft_mode: "full_rank"  # Options: "lora" (低秩微调), "full_rank" (质量门控参数全秩微调)
  lora:
    r: 16
    lora_alpha: 64
    lora_dropout: 0
    target_modules:
      - "quality_gate.gate"
  gpu_grab:
    grab: false
    memory_need_gb: 24
    over_grab: false
  # Quality gate parameters
  quality_loss_weight: 1
  quality_gate_init_mean: 0.0
  quality_gate_init_std: 0.02
  # Quality loss configuration
  quality_loss_type: "linear"  # 线性损失（直接使用sigmoid后的good_ratio）
  quality_loss_debug: false  # Enable debug output for quality loss computation
  quality_loss_params:
    # Loss averaging strategy
    sample_wise_averaging: true  # false: token-wise averaging, true: sample-wise averaging (avoid long sequence bias)
    # Full sequence prediction settings
    full_sequence_prediction: true   # Enable full sequence prediction (vs assistant-only SFT)
    mask_special_tokens: true        # Mask special format tokens like <|user|>, <|assistant|>
