# Select-MoE: åŸºäºæ··åˆä¸“å®¶æ¨¡å‹çš„æ•°æ®é€‰æ‹©ç­–ç•¥

## é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ•°æ®é€‰æ‹©å®éªŒï¼Œæ—¨åœ¨æ¢ç´¢ä¸€ç§åˆ©ç”¨æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMixture-of-Experts, MoEï¼‰è¿›è¡Œé«˜æ•ˆæ•°æ®ç­›é€‰çš„æ–¹æ³•ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

1. **é¢„çƒ­é˜¶æ®µ**: å¯¹å°å‹ Select-MoE æ¨¡å‹çš„ Routerï¼ˆè·¯ç”±å™¨ï¼‰è¿›è¡Œé¢„çƒ­å¾®è°ƒï¼Œä½¿å…¶å…·å¤‡æ•°æ®è´¨é‡åˆ¤åˆ«èƒ½åŠ›
2. **é€‰æ‹©é˜¶æ®µ**: åˆ©ç”¨é¢„çƒ­çš„ Router ä¸ºå¤§è§„æ¨¡æ•°æ®é›†æ‰“åˆ†ï¼Œç­›é€‰é«˜è´¨é‡æ•°æ®å­é›†  
3. **å¾®è°ƒé˜¶æ®µ**: ä½¿ç”¨ç­›é€‰çš„é«˜è´¨é‡æ•°æ®å¾®è°ƒå¤§è§„æ¨¡ç›®æ ‡æ¨¡å‹
4. **è¯„ä¼°é˜¶æ®µ**: è¯„ä¼°æ•°æ®é€‰æ‹©ç­–ç•¥çš„æœ€ç»ˆæ•ˆæœ

## âœ¨ æ ¸å¿ƒåˆ›æ–°

### Select-MoE æ¶æ„ç‰¹æ€§
- **ä¸¤å±‚è·¯ç”±æ¶æ„**: å®ç°è´¨é‡é—¨ + MoE + åƒåœ¾ä¸“å®¶çš„å¹¶è¡Œå¤„ç†ç»“æ„
- **è´¨é‡é—¨æœºåˆ¶**: ä¸€çº§è·¯ç”±è¿›è¡ŒäºŒå…ƒåˆ†ç±»ï¼Œåˆ¤æ–­æ•°æ®è´¨é‡ï¼ˆå¥½ vs åï¼‰
- **æ ‡å‡†MoEé›†æˆ**: ä½¿ç”¨æ ‡å‡†OlmoeSparseMoeBlockè¿›è¡Œä¸“å®¶è·¯ç”±ï¼Œä¿æŒåŸå§‹æƒé‡ä¸å˜
- **å¯é…ç½®åƒåœ¾ä¸“å®¶**: æ”¯æŒå¤šç§è¾“å‡ºæ¨¡å¼ï¼ˆé›¶å€¼ã€å™ªå£°ã€è‡ªå®šä¹‰ï¼‰å¤„ç†ä½è´¨é‡æ•°æ®
- **è´¨é‡åˆ†ç±»æŸå¤±**: ä½¿ç”¨sigmoid(good_ratio)æŸå¤±å‡½æ•°æ›¿ä»£Betaåˆ†å¸ƒçº¦æŸ
- **HuggingFace å…¼å®¹**: æ”¯æŒæ ‡å‡†çš„ `from_pretrained()` åŠ è½½å’Œç”Ÿæ€å·¥å…·

### æ¨¡å‹å¯¹æ¯”

| ç‰¹æ€§ | åŸå§‹ OLMoE | Select-MoE |
|------|------------|------------|
| MoEä¸“å®¶æ•° | 64 | 64 (ä¿æŒä¸å˜) |
| è´¨é‡é—¨ | æ—  | 2è¾“å‡º (å¥½/ååˆ†ç±») |
| åƒåœ¾ä¸“å®¶ | æ—  | 1ä¸ª (å¯é…ç½®è¾“å‡ºæ¨¡å¼) |
| è·¯ç”±ç»“æ„ | å•å±‚MoE | ä¸¤å±‚ (è´¨é‡é—¨ + MoE) |
| é¢„è®­ç»ƒæƒé‡ | - | MoEæƒé‡å®Œå…¨ä¿æŒ |
| æ•°æ®è´¨é‡é€‰æ‹© | æ—  | åŸºäºè´¨é‡é—¨è¾“å‡º |

## ğŸ› ï¸ æŠ€æœ¯æ ˆ

- **åŒ…ç®¡ç†**: [`uv`](https://docs.astral.sh/uv/) - å¿«é€Ÿ Python åŒ…ç®¡ç†å™¨
- **æ·±åº¦å­¦ä¹ **: [`torch`](https://pytorch.org/) 2.6.0, [`transformers`](https://github.com/huggingface/transformers)
- **æ¨¡å‹å¾®è°ƒ**: [`peft`](https://github.com/huggingface/peft) - LoRA ç­‰é«˜æ•ˆå¾®è°ƒæ–¹æ³•
- **åˆ†å¸ƒå¼è®­ç»ƒ**: [`accelerate`](https://github.com/huggingface/accelerate) - å¤šGPU è®­ç»ƒæ”¯æŒ
- **é…ç½®ç®¡ç†**: [`hydra`](https://github.com/facebookresearch/hydra) - çµæ´»çš„é…ç½®ç³»ç»Ÿ
- **æ¨¡å‹è¯„ä¼°**: [`lm-eval`](https://github.com/EleutherAI/lm-evaluation-harness) - æ ‡å‡†è¯„æµ‹æ¡†æ¶

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå‡†å¤‡

```bash
# 1. å®‰è£… uv åŒ…ç®¡ç†å™¨
wget -qO- https://astral.sh/uv/install.sh | sh

# 2. åŒæ­¥é¡¹ç›®ä¾èµ–
uv sync

# 3. (å¯é€‰) é…ç½®å›½å†…é•œåƒæº
./tools/chsrc set uv

# 4. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate
```

### æ•°æ®å‡†å¤‡

```bash
# ä¸‹è½½è®­ç»ƒæ•°æ®é›†
wget https://hf-mirror.com/datasets/princeton-nlp/less_data/resolve/main/less-data.zip
unzip less-data.zip

# ä¸‹è½½è¯„ä¼°æ•°æ®é›†
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download hails/mmlu_no_train --repo-type dataset 
huggingface-cli download cais/mmlu --repo-type dataset

# ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
huggingface-cli download allenai/OLMoE-1B-7B-0125
huggingface-cli download meta-llama/Llama-2-7b-hf
```

## ğŸ“‹ å®Œæ•´æ‰§è¡Œæµç¨‹

æœ¬é¡¹ç›®åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š**æ¨¡å‹è½¬æ¢** å’Œ **å››é˜¶æ®µè®­ç»ƒæµç¨‹**ã€‚

### æ­¥éª¤ 0: æ¨¡å‹è½¬æ¢

é¦–å…ˆéœ€è¦å°† OLMoE é¢„è®­ç»ƒæ¨¡å‹è½¬æ¢ä¸º Select-MoE æ ¼å¼ï¼š

```bash
# åŸºæœ¬è½¬æ¢
python scripts/convert_olmoe_to_select_moe.py \
    --save-path ./converted_models/select_moe_converted_OLMoE-1B-7B-0125

# (å¯é€‰) éªŒè¯è½¬æ¢ç»“æœ
python scripts/compare_converted_model.py \
    --converted-model ./converted_models/select_moe_converted_OLMoE-1B-7B-0125 \
    --dtype bfloat16 \
```

### æ­¥éª¤ 1: é¢„çƒ­è®­ç»ƒ Select-MoE è·¯ç”±æƒé‡

è®­ç»ƒ Select-MoE æ¨¡å‹çš„ Routerï¼Œä½¿å…¶å­¦ä¹ æ•°æ®è´¨é‡åˆ¤åˆ«ï¼š

```bash
# å• GPU è®­ç»ƒ
CUDA_VISIBLE_DEVICES=0 bash scripts/run_stage_1.sh

# å¤š GPU è®­ç»ƒ
CUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/run_stage_1.sh

# è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ
CUDA_VISIBLE_DEVICES=0 bash scripts/run_stage_1.sh \
    training.learning_rate=5e-5 \
    training.batch_size=8 \
    dataset.subset_ratio=0.1
```

**è¾“å‡º**: æƒé‡æ–‡ä»¶ä¿å­˜åœ¨ `outputs/stage_1_pretrain/YYYY-MM-DD/HH-MM-SS/full_rank_weights.pt`

### æ­¥éª¤ 2: æ•°æ®é€‰æ‹©

ä½¿ç”¨é¢„çƒ­çš„ Select-MoE æ¨¡å‹ä¸ºè®­ç»ƒæ•°æ®æ‰“åˆ†å¹¶ç­›é€‰ï¼š

```bash
# ä½¿ç”¨é˜¶æ®µ1çš„è¾“å‡ºè¿›è¡Œæ•°æ®é€‰æ‹©
CUDA_VISIBLE_DEVICES=0 bash scripts/run_stage_2.sh \
    model_checkpoint_path=outputs/stage_1_pretrain/2025-07-16/01-57-27/full_rank_weights.pt

# è°ƒæ•´é€‰æ‹©æ¯”ä¾‹ï¼ˆé€‰æ‹©å‰10%çš„æ•°æ®ï¼‰
CUDA_VISIBLE_DEVICES=0 bash scripts/run_stage_2.sh \
    model_checkpoint_path=outputs/stage_1_pretrain/2025-07-16/01-57-27/full_rank_weights.pt \
    selection_percentage=0.1
```

**è¾“å‡º**: ç­›é€‰æ•°æ®ä¿å­˜åœ¨ `outputs/stage_2_selection/YYYY-MM-DD/HH-MM-SS/selected_data.jsonl`

### æ­¥éª¤ 3: ç›®æ ‡æ¨¡å‹å¾®è°ƒ

ä½¿ç”¨ç­›é€‰çš„æ•°æ®å¯¹ Llama-2-7B è¿›è¡Œ LoRA å¾®è°ƒï¼š

```bash
# åŸºæœ¬å¾®è°ƒ
CUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/run_stage_3.sh \
    dataset.data_path=outputs/stage_2_selection/2025-07-17/04-49-54/selected_data.jsonl

# è‡ªå®šä¹‰ LoRA å‚æ•°
CUDA_VISIBLE_DEVICES=0,1 bash scripts/run_stage_3.sh \
    dataset.data_path=outputs/stage_2_selection/2025-07-17/04-49-54/selected_data.jsonl \
    training.lora.r=64 \
    training.lora.lora_alpha=256
```

**è¾“å‡º**: LoRA é€‚é…å™¨ä¿å­˜åœ¨ `outputs/stage_3_finetune/YYYY-MM-DD/HH-MM-SS/checkpoint-XXXX/`

### æ­¥éª¤ 4: æ¨¡å‹è¯„ä¼°

ä½¿ç”¨ `lm-eval` è¯„ä¼°å¾®è°ƒåæ¨¡å‹çš„æ€§èƒ½ï¼š

```bash
# MMLU è¯„ä¼°
CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch -m lm_eval --model hf \
    --model_args pretrained=meta-llama/Llama-2-7b-hf,peft=outputs/stage_3_finetune/2025-07-18/01-01-10/checkpoint-1804 \
    --tasks mmlu \
    --batch_size auto \
    --output_path outputs/stage_4_eval

# å¤šä»»åŠ¡è¯„ä¼°
CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch -m lm_eval --model hf \
    --model_args pretrained=meta-llama/Llama-2-7b-hf,peft=outputs/stage_3_finetune/2025-07-18/01-01-10/checkpoint-1804 \
    --tasks mmlu,hellaswag,arc_easy,arc_challenge \
    --batch_size auto \
    --output_path outputs/stage_4_eval
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### ç¨‹åºåŒ–ä½¿ç”¨ Select-MoE æ¨¡å‹

```python
from src.models.select_moe import SelectMoeForCausalLM, register_select_moe

# æ³¨å†Œ Select-MoEï¼ˆå¿…é¡»åœ¨åŠ è½½å‰æ‰§è¡Œï¼‰
register_select_moe()

# åŠ è½½è½¬æ¢åçš„æ¨¡å‹
model = SelectMoeForCausalLM.from_pretrained("./converted_models/select_moe_converted_OLMoE-1B-7B-0125")

# è®­ç»ƒæ—¶å¼€å¯ router logits
outputs = model(
    input_ids=input_ids,
    attention_mask=attention_mask,
    labels=labels,
    output_router_logits=True  # é‡è¦ï¼šè®­ç»ƒæ—¶å¿…é¡»ä¸ºTrue
)

# æ–°æ¶æ„è¿”å›å­—å…¸æ ¼å¼çš„è·¯ç”±è¾“å‡º
for layer_output in outputs.router_logits:
    quality_logits = layer_output["quality_logits"]  # å½¢çŠ¶: [batch, seq_len, 2]
    moe_logits = layer_output["moe_logits"]          # å½¢çŠ¶: [batch*seq_len, num_experts]

# æŸå¤±åŒ…å«è¯­è¨€å»ºæ¨¡ + è´Ÿè½½å‡è¡¡ + è´¨é‡åˆ†ç±»æŸå¤±
total_loss = outputs.loss
```

### å‚æ•°è¦†å†™æœºåˆ¶

é¡¹ç›®ä½¿ç”¨ Hydra é…ç½®ç®¡ç†ï¼Œæ”¯æŒçµæ´»çš„å‘½ä»¤è¡Œå‚æ•°è¦†å†™ï¼š

```bash
# åŸºæœ¬è¯­æ³•
bash scripts/script_name.sh key1=value1 key2.subkey=value2

# å®é™…ç¤ºä¾‹
bash scripts/run_stage_1.sh training.learning_rate=1e-5 dataset.subset_ratio=0.1
bash scripts/run_stage_2.sh selection_percentage=0.1 data_process.batch_size=32
bash scripts/run_stage_3.sh training.lora.r=128 training.batch_size=64
```

## âš™ï¸ é…ç½®è¯´æ˜

### ä¸»è¦é…ç½®æ–‡ä»¶

- `configs/stage_1_pretrain.yaml` - é˜¶æ®µ1é¢„çƒ­è®­ç»ƒé…ç½®
- `configs/stage_2_selection.yaml` - é˜¶æ®µ2æ•°æ®é€‰æ‹©é…ç½®  
- `configs/stage_3_finetune.yaml` - é˜¶æ®µ3æ¨¡å‹å¾®è°ƒé…ç½®
- `configs/stage_4_evaluate.yaml` - é˜¶æ®µ4æ¨¡å‹è¯„ä¼°é…ç½®

### å…³é”®å‚æ•°è¯´æ˜

**é˜¶æ®µ1 (é¢„çƒ­è®­ç»ƒ)**:
- `training.peft_mode`: è®­ç»ƒæ¨¡å¼ (`full_rank` æˆ– `lora`)
- `training.learning_rate`: å­¦ä¹ ç‡ (é»˜è®¤: 1e-4)
- `dataset.subset_ratio`: è®­ç»ƒæ•°æ®æ¯”ä¾‹ (é»˜è®¤: 0.05)
- `training.quality_loss_weight`: è´¨é‡åˆ†ç±»æŸå¤±æƒé‡ (é»˜è®¤: 0.01)
- `training.quality_gate_init_mean/std`: è´¨é‡é—¨åˆå§‹åŒ–å‚æ•°
- `training.trash_expert_mode`: åƒåœ¾ä¸“å®¶æ¨¡å¼ ("zero", "noise", "custom")
- `training.enable_load_balancing`: å¯ç”¨MoEè´Ÿè½½å‡è¡¡æŸå¤±

**é˜¶æ®µ2 (æ•°æ®é€‰æ‹©)**:
- `selection_percentage`: æ•°æ®é€‰æ‹©æ¯”ä¾‹ (é»˜è®¤: 0.05)
- `model_checkpoint_path`: é˜¶æ®µ1è¾“å‡ºçš„æƒé‡è·¯å¾„

**é˜¶æ®µ3 (æ¨¡å‹å¾®è°ƒ)**:
- `training.lora.r`: LoRA ç§© (é»˜è®¤: 128)
- `training.learning_rate`: å­¦ä¹ ç‡ (é»˜è®¤: 2e-5)
- `dataset.data_path`: é˜¶æ®µ2è¾“å‡ºçš„æ•°æ®è·¯å¾„

## ğŸ’¡ é‡è¦æç¤º

### ç¯å¢ƒå˜é‡
æ¯ä¸ªè„šæœ¬æ‰§è¡Œå‰éƒ½éœ€è¦è®¾ç½® `CUDA_VISIBLE_DEVICES`ï¼š
```bash
export CUDA_VISIBLE_DEVICES=0,1,2,3  # æŒ‡å®šä½¿ç”¨çš„GPU
```

### å†…å­˜éœ€æ±‚
- **é˜¶æ®µ1**: Select-MoE å…¨å‚æ•°è®­ç»ƒï¼Œå»ºè®®è‡³å°‘ 16GB GPU å†…å­˜
- **é˜¶æ®µ2**: æ•°æ®é€‰æ‹©æ¨ç†ï¼Œå†…å­˜éœ€æ±‚è¾ƒå°
- **é˜¶æ®µ3**: Llama-2-7B LoRA è®­ç»ƒï¼Œå»ºè®® 24GB ä»¥ä¸Š GPU å†…å­˜

### è·¯å¾„ä¾èµ–
æ³¨æ„å„é˜¶æ®µä¹‹é—´çš„è·¯å¾„ä¾èµ–å…³ç³»ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è¾“å…¥è·¯å¾„ï¼š
- é˜¶æ®µ2 éœ€è¦é˜¶æ®µ1 çš„ `full_rank_weights.pt`
- é˜¶æ®µ3 éœ€è¦é˜¶æ®µ2 çš„ `selected_data.jsonl`  
- é˜¶æ®µ4 éœ€è¦é˜¶æ®µ3 çš„ LoRA æ£€æŸ¥ç‚¹

## ğŸ“– è¯¦ç»†æ–‡æ¡£

æ›´å¤šè¯¦ç»†çš„æ‰§è¡Œè¯´æ˜å’Œå‚æ•°é…ç½®ï¼Œè¯·å‚è€ƒ [`docs.md`](docs.md) æ–‡ä»¶ã€‚

## ğŸ”¬ æŠ€æœ¯åŸç†

### ä¸¤å±‚è·¯ç”±æ¶æ„è®¾è®¡

Select-MoEé‡‡ç”¨åˆ›æ–°çš„ä¸¤å±‚è·¯ç”±æ¶æ„ï¼Œå®ç°æ›´ç²¾ç¡®çš„æ•°æ®è´¨é‡åˆ¤åˆ«ï¼š

**ç¬¬ä¸€å±‚ï¼šè´¨é‡é—¨ (Quality Gate)**
- è¿›è¡ŒäºŒå…ƒåˆ†ç±»ï¼šå¥½æ•°æ® vs åæ•°æ®
- è¾“å‡ºgood_ratioå’Œbad_ratioï¼Œæ§åˆ¶åç»­å¤„ç†æƒé‡
- ä½¿ç”¨å°å‹å…¨è¿æ¥ç½‘ç»œï¼Œè®¡ç®—æ•ˆç‡é«˜

**ç¬¬äºŒå±‚ï¼šå¹¶è¡Œå¤„ç†**
```
y = good_ratio * y_normal + bad_ratio * y_trash
```
- **æ­£å¸¸è·¯å¾„**: é€šè¿‡æ ‡å‡†MoEå¤„ç†ï¼Œä¿æŒåŸå§‹OLMoEèƒ½åŠ›
- **åƒåœ¾è·¯å¾„**: é€šè¿‡åƒåœ¾ä¸“å®¶å¤„ç†ï¼Œå¯é…ç½®è¾“å‡ºæ¨¡å¼
- **åŠ æƒç»„åˆ**: åŸºäºè´¨é‡é—¨è¾“å‡ºåŠ¨æ€ç»„åˆä¸¤è·¯ç»“æœ

### è´¨é‡åˆ†ç±»æŸå¤±

æ–°æ¶æ„ä½¿ç”¨ç®€æ´çš„è´¨é‡åˆ†ç±»æŸå¤±ï¼š
```
L_quality = sigmoid(good_ratio).mean()
```

**æŸå¤±ç‰¹æ€§**ï¼š
- **è®¡ç®—ç²’åº¦**: å¯¹æ¯ä¸ªtokenåœ¨æ¯ä¸ªå±‚åˆ†åˆ«è®¡ç®—
- **ä¼˜åŒ–ç›®æ ‡**: é¼“åŠ±æ¨¡å‹å­¦ä¹ åŒºåˆ†æ•°æ®è´¨é‡
- **æ•°å€¼ç¨³å®š**: ä½¿ç”¨sigmoidæ¿€æ´»ï¼Œé¿å…æ•°å€¼é—®é¢˜

**æ€»æŸå¤±æ„æˆ**ï¼š
```
L_total = L_language_modeling + w_load_balancing * L_load_balancing + w_quality * L_quality
```

### åƒåœ¾ä¸“å®¶æœºåˆ¶

**è¾“å‡ºæ¨¡å¼**ï¼š
- **zeroæ¨¡å¼**: è¾“å‡ºé›¶å‘é‡ï¼Œæä¾›æœ€å°å¹²æ‰°
- **noiseæ¨¡å¼**: è¾“å‡ºä¸è¾“å…¥åŒåˆ†å¸ƒçš„å™ªå£°
- **customæ¨¡å¼**: æ”¯æŒè‡ªå®šä¹‰è¡Œä¸ºæ‰©å±•

**è®¾è®¡ä¼˜åŠ¿**ï¼š
1. **æ¨¡å—åŒ–**: åƒåœ¾ä¸“å®¶ç‹¬ç«‹äºMoEï¼Œæ˜“äºè°ƒè¯•å’Œä¼˜åŒ–
2. **å¯é…ç½®**: æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„è¾“å‡ºæ¨¡å¼
3. **é«˜æ•ˆ**: é¿å…äº†å¤æ‚çš„ä¸“å®¶æ‰©å±•å’Œæƒé‡ç®¡ç†

## ğŸš§ å¼€å‘è®¡åˆ’

1. **åƒåœ¾æ¡¶ä¸“å®¶ä¼˜åŒ–**: å®ç°æ›´æ™ºèƒ½çš„åƒåœ¾æ¡¶ä¸“å®¶åˆå§‹åŒ–å’Œè¡Œä¸ºç­–ç•¥
2. **å¤šä»»åŠ¡é€‚é…**: æ‰©å±•æ”¯æŒæ›´å¤šä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®é€‰æ‹©
3. **æ•ˆç‡ä¼˜åŒ–**: ä¼˜åŒ–è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼Œæ”¯æŒæ›´å¤§è§„æ¨¡æ¨¡å‹
4. **è¯„ä¼°æ‰©å±•**: å¢åŠ æ›´å¤šè¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•

