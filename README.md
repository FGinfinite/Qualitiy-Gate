# Select-MoE: åŸºäºæ··åˆä¸“å®¶æ¨¡å‹çš„æ•°æ®é€‰æ‹©ç­–ç•¥

## é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåˆ›æ–°çš„æ•°æ®é€‰æ‹©å®éªŒï¼Œæ—¨åœ¨æ¢ç´¢ä¸€ç§åˆ©ç”¨æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMixture-of-Experts, MoEï¼‰è¿›è¡Œé«˜æ•ˆæ•°æ®ç­›é€‰çš„æ–¹æ³•ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š

1. **é¢„çƒ­é˜¶æ®µ**: å¯¹å°å‹ Select-MoE æ¨¡å‹çš„ Routerï¼ˆè·¯ç”±å™¨ï¼‰è¿›è¡Œé¢„çƒ­å¾®è°ƒï¼Œä½¿å…¶å…·å¤‡æ•°æ®è´¨é‡åˆ¤åˆ«èƒ½åŠ›
2. **é€‰æ‹©é˜¶æ®µ**: åˆ©ç”¨é¢„çƒ­çš„ Router ä¸ºå¤§è§„æ¨¡æ•°æ®é›†æ‰“åˆ†ï¼Œç­›é€‰é«˜è´¨é‡æ•°æ®å­é›†  
3. **å¾®è°ƒé˜¶æ®µ**: ä½¿ç”¨ç­›é€‰çš„é«˜è´¨é‡æ•°æ®å¾®è°ƒå¤§è§„æ¨¡ç›®æ ‡æ¨¡å‹
4. **è¯„ä¼°é˜¶æ®µ**: è¯„ä¼°æ•°æ®é€‰æ‹©ç­–ç•¥çš„æœ€ç»ˆæ•ˆæœ

## âœ¨ æ ¸å¿ƒåˆ›æ–°

### Select-MoE æ¶æ„ç‰¹æ€§
- **åƒåœ¾æ¡¶ä¸“å®¶æœºåˆ¶**: åŠ¨æ€æ·»åŠ "åƒåœ¾æ¡¶ä¸“å®¶"ï¼Œå¯¹ä½è´¨é‡æ•°æ®æä¾›è´Ÿå‘æ¿€åŠ±
- **æƒé‡ä¿æŒ**: å®Œç¾ä¿æŒåŸå§‹ OLMoE é¢„è®­ç»ƒæƒé‡ï¼Œä»…æ–°å¢åƒåœ¾æ¡¶ä¸“å®¶ç»´åº¦
- **çº¦æŸæŸå¤±**: åŸºäº Beta åˆ†å¸ƒçš„è‡ªå®šä¹‰çº¦æŸæŸå¤±ï¼Œå¼•å¯¼ Router å­¦ä¹ æ•°æ®è´¨é‡åŒºåˆ†
- **HuggingFace å…¼å®¹**: æ”¯æŒæ ‡å‡†çš„ `from_pretrained()` åŠ è½½å’Œç”Ÿæ€å·¥å…·

### æ¨¡å‹å¯¹æ¯”

| ç‰¹æ€§ | åŸå§‹ OLMoE | Select-MoE |
|------|------------|------------|
| åŸå§‹ä¸“å®¶æ•° | 64 | 64 (ä¿æŒä¸å˜) |
| åƒåœ¾æ¡¶ä¸“å®¶æ•° | 0 | 8 (= top_k) |
| Gate è¾“å‡ºç»´åº¦ | [64, hidden_size] | [72, hidden_size] |
| é¢„è®­ç»ƒæƒé‡ | - | å®Œå…¨ä¿æŒ |
| æ•°æ®è´¨é‡é€‰æ‹© | æ—  | åŠ¨æ€è·¯ç”±åˆ°åƒåœ¾æ¡¶ |

## ğŸ› ï¸ æŠ€æœ¯æ ˆ

- **åŒ…ç®¡ç†**: [`uv`](https://docs.astral.sh/uv/) - å¿«é€Ÿ Python åŒ…ç®¡ç†å™¨
- **æ·±åº¦å­¦ä¹ **: [`torch`](https://pytorch.org/) 2.6.0, [`transformers`](https://github.com/huggingface/transformers)
- **æ¨¡å‹å¾®è°ƒ**: [`peft`](https://github.com/huggingface/peft) - LoRA ç­‰é«˜æ•ˆå¾®è°ƒæ–¹æ³•
- **åˆ†å¸ƒå¼è®­ç»ƒ**: [`accelerate`](https://github.com/huggingface/accelerate) - å¤šGPU è®­ç»ƒæ”¯æŒ
- **é…ç½®ç®¡ç†**: [`hydra`](https://github.com/facebookresearch/hydra) - çµæ´»çš„é…ç½®ç³»ç»Ÿ
- **æ¨¡å‹è¯„ä¼°**: [`lm-eval`](https://github.com/EleutherAI/lm-evaluation-harness) - æ ‡å‡†è¯„æµ‹æ¡†æ¶

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå‡†å¤‡

```bash
# 1. å®‰è£… uv åŒ…ç®¡ç†å™¨
wget -qO- https://astral.sh/uv/install.sh | sh

# 2. åŒæ­¥é¡¹ç›®ä¾èµ–
uv sync

# 3. (å¯é€‰) é…ç½®å›½å†…é•œåƒæº
./tools/chsrc set uv

# 4. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source .venv/bin/activate
```

### æ•°æ®å‡†å¤‡

```bash
# ä¸‹è½½è®­ç»ƒæ•°æ®é›†
wget https://hf-mirror.com/datasets/princeton-nlp/less_data/resolve/main/less-data.zip
unzip less-data.zip

# ä¸‹è½½è¯„ä¼°æ•°æ®é›†
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download hails/mmlu_no_train --repo-type dataset 
huggingface-cli download cais/mmlu --repo-type dataset

# ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
huggingface-cli download allenai/OLMoE-1B-7B-0125
huggingface-cli download meta-llama/Llama-2-7b-hf
```

## ğŸ“‹ å®Œæ•´æ‰§è¡Œæµç¨‹

æœ¬é¡¹ç›®åŒ…å«ä¸¤ä¸ªä¸»è¦éƒ¨åˆ†ï¼š**æ¨¡å‹è½¬æ¢** å’Œ **å››é˜¶æ®µè®­ç»ƒæµç¨‹**ã€‚

### æ­¥éª¤ 0: æ¨¡å‹è½¬æ¢

é¦–å…ˆéœ€è¦å°† OLMoE é¢„è®­ç»ƒæ¨¡å‹è½¬æ¢ä¸º Select-MoE æ ¼å¼ï¼š

```bash
# åŸºæœ¬è½¬æ¢
python scripts/convert_olmoe_to_select_moe.py \
    --save-path ./converted_models/select_moe_converted_OLMoE-1B-7B-0125

# (å¯é€‰) éªŒè¯è½¬æ¢ç»“æœ
python scripts/compare_converted_model.py \
    --converted-model ./converted_models/select_moe_converted_OLMoE-1B-7B-0125 \
    --dtype bfloat16 \
```

### æ­¥éª¤ 1: é¢„çƒ­è®­ç»ƒ Select-MoE è·¯ç”±æƒé‡

è®­ç»ƒ Select-MoE æ¨¡å‹çš„ Routerï¼Œä½¿å…¶å­¦ä¹ æ•°æ®è´¨é‡åˆ¤åˆ«ï¼š

```bash
# å• GPU è®­ç»ƒ
CUDA_VISIBLE_DEVICES=0 bash scripts/run_stage_1.sh

# å¤š GPU è®­ç»ƒ
CUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/run_stage_1.sh

# è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ
CUDA_VISIBLE_DEVICES=0 bash scripts/run_stage_1.sh \
    training.learning_rate=5e-5 \
    training.batch_size=8 \
    dataset.subset_ratio=0.1
```

**è¾“å‡º**: æƒé‡æ–‡ä»¶ä¿å­˜åœ¨ `outputs/stage_1_pretrain/YYYY-MM-DD/HH-MM-SS/full_rank_weights.pt`

### æ­¥éª¤ 2: æ•°æ®é€‰æ‹©

ä½¿ç”¨é¢„çƒ­çš„ Select-MoE æ¨¡å‹ä¸ºè®­ç»ƒæ•°æ®æ‰“åˆ†å¹¶ç­›é€‰ï¼š

```bash
# ä½¿ç”¨é˜¶æ®µ1çš„è¾“å‡ºè¿›è¡Œæ•°æ®é€‰æ‹©
CUDA_VISIBLE_DEVICES=0 bash scripts/run_stage_2.sh \
    model_checkpoint_path=outputs/stage_1_pretrain/2025-07-16/01-57-27/full_rank_weights.pt

# è°ƒæ•´é€‰æ‹©æ¯”ä¾‹ï¼ˆé€‰æ‹©å‰10%çš„æ•°æ®ï¼‰
CUDA_VISIBLE_DEVICES=0 bash scripts/run_stage_2.sh \
    model_checkpoint_path=outputs/stage_1_pretrain/2025-07-16/01-57-27/full_rank_weights.pt \
    selection_percentage=0.1
```

**è¾“å‡º**: ç­›é€‰æ•°æ®ä¿å­˜åœ¨ `outputs/stage_2_selection/YYYY-MM-DD/HH-MM-SS/selected_data.jsonl`

### æ­¥éª¤ 3: ç›®æ ‡æ¨¡å‹å¾®è°ƒ

ä½¿ç”¨ç­›é€‰çš„æ•°æ®å¯¹ Llama-2-7B è¿›è¡Œ LoRA å¾®è°ƒï¼š

```bash
# åŸºæœ¬å¾®è°ƒ
CUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/run_stage_3.sh \
    dataset.data_path=outputs/stage_2_selection/2025-07-17/04-49-54/selected_data.jsonl

# è‡ªå®šä¹‰ LoRA å‚æ•°
CUDA_VISIBLE_DEVICES=0,1 bash scripts/run_stage_3.sh \
    dataset.data_path=outputs/stage_2_selection/2025-07-17/04-49-54/selected_data.jsonl \
    training.lora.r=64 \
    training.lora.lora_alpha=256
```

**è¾“å‡º**: LoRA é€‚é…å™¨ä¿å­˜åœ¨ `outputs/stage_3_finetune/YYYY-MM-DD/HH-MM-SS/checkpoint-XXXX/`

### æ­¥éª¤ 4: æ¨¡å‹è¯„ä¼°

ä½¿ç”¨ `lm-eval` è¯„ä¼°å¾®è°ƒåæ¨¡å‹çš„æ€§èƒ½ï¼š

```bash
# MMLU è¯„ä¼°
CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch -m lm_eval --model hf \
    --model_args pretrained=meta-llama/Llama-2-7b-hf,peft=outputs/stage_3_finetune/2025-07-18/01-01-10/checkpoint-1804 \
    --tasks mmlu \
    --batch_size auto \
    --output_path outputs/stage_4_eval

# å¤šä»»åŠ¡è¯„ä¼°
CUDA_VISIBLE_DEVICES=0,1,2,3 accelerate launch -m lm_eval --model hf \
    --model_args pretrained=meta-llama/Llama-2-7b-hf,peft=outputs/stage_3_finetune/2025-07-18/01-01-10/checkpoint-1804 \
    --tasks mmlu,hellaswag,arc_easy,arc_challenge \
    --batch_size auto \
    --output_path outputs/stage_4_eval
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### ç¨‹åºåŒ–ä½¿ç”¨ Select-MoE æ¨¡å‹

```python
from src.models.select_moe import SelectMoeForCausalLM, register_select_moe

# æ³¨å†Œ Select-MoEï¼ˆå¿…é¡»åœ¨åŠ è½½å‰æ‰§è¡Œï¼‰
register_select_moe()

# åŠ è½½è½¬æ¢åçš„æ¨¡å‹
model = SelectMoeForCausalLM.from_pretrained("./converted_models/select_moe_converted_OLMoE-1B-7B-0125")

# è®­ç»ƒæ—¶å¼€å¯ router logits
outputs = model(
    input_ids=input_ids,
    attention_mask=attention_mask,
    labels=labels,
    output_router_logits=True  # é‡è¦ï¼šè®­ç»ƒæ—¶å¿…é¡»ä¸ºTrue
)

# æŸå¤±åŒ…å«æ‰€æœ‰ç»„ä»¶
total_loss = outputs.loss  # è¯­è¨€å»ºæ¨¡æŸå¤± + è´Ÿè½½å‡è¡¡æŸå¤± + çº¦æŸæŸå¤±
```

### å‚æ•°è¦†å†™æœºåˆ¶

é¡¹ç›®ä½¿ç”¨ Hydra é…ç½®ç®¡ç†ï¼Œæ”¯æŒçµæ´»çš„å‘½ä»¤è¡Œå‚æ•°è¦†å†™ï¼š

```bash
# åŸºæœ¬è¯­æ³•
bash scripts/script_name.sh key1=value1 key2.subkey=value2

# å®é™…ç¤ºä¾‹
bash scripts/run_stage_1.sh training.learning_rate=1e-5 dataset.subset_ratio=0.1
bash scripts/run_stage_2.sh selection_percentage=0.1 data_process.batch_size=32
bash scripts/run_stage_3.sh training.lora.r=128 training.batch_size=64
```

## âš™ï¸ é…ç½®è¯´æ˜

### ä¸»è¦é…ç½®æ–‡ä»¶

- `configs/stage_1_pretrain.yaml` - é˜¶æ®µ1é¢„çƒ­è®­ç»ƒé…ç½®
- `configs/stage_2_selection.yaml` - é˜¶æ®µ2æ•°æ®é€‰æ‹©é…ç½®  
- `configs/stage_3_finetune.yaml` - é˜¶æ®µ3æ¨¡å‹å¾®è°ƒé…ç½®
- `configs/stage_4_evaluate.yaml` - é˜¶æ®µ4æ¨¡å‹è¯„ä¼°é…ç½®

### å…³é”®å‚æ•°è¯´æ˜

**é˜¶æ®µ1 (é¢„çƒ­è®­ç»ƒ)**:
- `training.peft_mode`: è®­ç»ƒæ¨¡å¼ (`full_rank` æˆ– `lora`)
- `training.learning_rate`: å­¦ä¹ ç‡ (é»˜è®¤: 1e-4)
- `dataset.subset_ratio`: è®­ç»ƒæ•°æ®æ¯”ä¾‹ (é»˜è®¤: 0.05)

**é˜¶æ®µ2 (æ•°æ®é€‰æ‹©)**:
- `selection_percentage`: æ•°æ®é€‰æ‹©æ¯”ä¾‹ (é»˜è®¤: 0.05)
- `model_checkpoint_path`: é˜¶æ®µ1è¾“å‡ºçš„æƒé‡è·¯å¾„

**é˜¶æ®µ3 (æ¨¡å‹å¾®è°ƒ)**:
- `training.lora.r`: LoRA ç§© (é»˜è®¤: 128)
- `training.learning_rate`: å­¦ä¹ ç‡ (é»˜è®¤: 2e-5)
- `dataset.data_path`: é˜¶æ®µ2è¾“å‡ºçš„æ•°æ®è·¯å¾„

## ğŸ’¡ é‡è¦æç¤º

### ç¯å¢ƒå˜é‡
æ¯ä¸ªè„šæœ¬æ‰§è¡Œå‰éƒ½éœ€è¦è®¾ç½® `CUDA_VISIBLE_DEVICES`ï¼š
```bash
export CUDA_VISIBLE_DEVICES=0,1,2,3  # æŒ‡å®šä½¿ç”¨çš„GPU
```

### å†…å­˜éœ€æ±‚
- **é˜¶æ®µ1**: Select-MoE å…¨å‚æ•°è®­ç»ƒï¼Œå»ºè®®è‡³å°‘ 16GB GPU å†…å­˜
- **é˜¶æ®µ2**: æ•°æ®é€‰æ‹©æ¨ç†ï¼Œå†…å­˜éœ€æ±‚è¾ƒå°
- **é˜¶æ®µ3**: Llama-2-7B LoRA è®­ç»ƒï¼Œå»ºè®® 24GB ä»¥ä¸Š GPU å†…å­˜

### è·¯å¾„ä¾èµ–
æ³¨æ„å„é˜¶æ®µä¹‹é—´çš„è·¯å¾„ä¾èµ–å…³ç³»ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è¾“å…¥è·¯å¾„ï¼š
- é˜¶æ®µ2 éœ€è¦é˜¶æ®µ1 çš„ `full_rank_weights.pt`
- é˜¶æ®µ3 éœ€è¦é˜¶æ®µ2 çš„ `selected_data.jsonl`  
- é˜¶æ®µ4 éœ€è¦é˜¶æ®µ3 çš„ LoRA æ£€æŸ¥ç‚¹

## ğŸ“– è¯¦ç»†æ–‡æ¡£

æ›´å¤šè¯¦ç»†çš„æ‰§è¡Œè¯´æ˜å’Œå‚æ•°é…ç½®ï¼Œè¯·å‚è€ƒ [`docs.md`](docs.md) æ–‡ä»¶ã€‚

## ğŸ”¬ æŠ€æœ¯åŸç†

ä¸ºäº†å¼•å¯¼Routerï¼ˆè·¯ç”±å™¨ï¼‰å­¦ä¹ åŒºåˆ†é«˜è´¨é‡å’Œä½è´¨é‡æ•°æ®ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç‰¹æ®Šçš„çº¦æŸæŸå¤±å‡½æ•°ã€‚è¯¥æŸå¤±å‡½æ•°çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå¯¹äºä¸€ä¸ªç»™å®šçš„è¾“å…¥ï¼Œæˆ‘ä»¬æœŸæœ›Routerçš„Top-Kä¸“å®¶é€‰æ‹©æ¦‚ç‡ä¹‹å’Œï¼ˆ`ratio`ï¼‰æ¥è¿‘äº1ï¼Œè€Œ"åƒåœ¾æ¡¶"ä¸“å®¶çš„é€‰æ‹©æ¦‚ç‡ä¹‹å’Œï¼ˆ`1 - ratio`ï¼‰æ¥è¿‘äº0ã€‚

è¿™ç§æœºåˆ¶é€šè¿‡ä¸€ä¸ªå®šåˆ¶çš„æŸå¤±å‡½æ•°æ¥å®ç°ï¼Œè¯¥å‡½æ•°å—åˆ°Betaåˆ†å¸ƒçš„å¯å‘ï¼Œæ—¨åœ¨å°† `ratio` å€¼æ¨å‘1ã€‚

### çº¦æŸæŸå¤±è®¾è®¡

çº¦æŸæŸå¤± `L_constraint` çš„è®¡ç®—æ–¹å¼å¦‚ä¸‹ï¼š

```
L_constraint = -((Î± - 1) * log(ratio) + (Î² - 1) * log(1 - ratio))
```

å…¶ä¸­ï¼š
- `ratio` æ˜¯ Top-K ä¸“å®¶çš„é€‰æ‹©æ¦‚ç‡ä¹‹å’Œ
- `Î±` (`trash_can_loss_alpha`) å’Œ `Î²` (`trash_can_loss_beta`) æ˜¯æ§åˆ¶æŸå¤±å‡½æ•°å½¢æ€çš„å‚æ•°
- åœ¨å®ç°ä¸­ï¼Œ`Î±` å›ºå®šä¸º1ï¼Œä¸“æ³¨äº `Î²` çš„å½±å“

æ€»æŸå¤±ä¸ºï¼š
```
L_total = L_ce + w_constraint * L_constraint
```

### æ ¸å¿ƒå‚æ•°

- **`constraint_loss_weight`**: çº¦æŸæŸå¤±åœ¨æ€»æŸå¤±ä¸­çš„æƒé‡ï¼Œæ§åˆ¶æ•°æ®è´¨é‡ç­›é€‰çš„é‡è§†ç¨‹åº¦
- **`trash_can_loss_beta`**: æ§åˆ¶å¯¹"åƒåœ¾æ¡¶"ä¸“å®¶æ¿€æ´»çš„æƒ©ç½šåŠ›åº¦

### "åƒåœ¾æ¡¶"ä¸“å®¶æœºåˆ¶

1. **åŠ¨æ€æ•°é‡**: åƒåœ¾æ¡¶ä¸“å®¶æ•°é‡ç­‰äº top-k æ¿€æ´»æ•°é‡
2. **è¡Œä¸ºç‰¹ç‚¹**: è¾“å‡ºå…¨é›¶å‘é‡ï¼Œæä¾›è´Ÿå‘æ¿€åŠ±
3. **åˆå§‹åŒ–ç­–ç•¥**: é€šè¿‡æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–ï¼Œå‚æ•°å¯é…ç½®

## ğŸš§ å¼€å‘è®¡åˆ’

1. **åƒåœ¾æ¡¶ä¸“å®¶ä¼˜åŒ–**: å®ç°æ›´æ™ºèƒ½çš„åƒåœ¾æ¡¶ä¸“å®¶åˆå§‹åŒ–å’Œè¡Œä¸ºç­–ç•¥
2. **å¤šä»»åŠ¡é€‚é…**: æ‰©å±•æ”¯æŒæ›´å¤šä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®é€‰æ‹©
3. **æ•ˆç‡ä¼˜åŒ–**: ä¼˜åŒ–è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ï¼Œæ”¯æŒæ›´å¤§è§„æ¨¡æ¨¡å‹
4. **è¯„ä¼°æ‰©å±•**: å¢åŠ æ›´å¤šè¯„ä¼°æŒ‡æ ‡å’ŒåŸºå‡†æµ‹è¯•

