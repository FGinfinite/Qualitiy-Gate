# **权威指南：针对含自定义模块模型的LoRA微调最佳实践与工作流**





## **1. 概念框架：“基础模型固化”原则**



在深入探讨具体的技术实现之前，必须首先建立一个核心的指导原则：**一个旨在解耦架构修改与参数高效适配的两阶段流程**。您所面临的核心挑战——在对自定义模块进行LoRA微调时产生的保存与加载矛盾——其根源在于一个普遍的思维误区。这个误区是将模型的架构修改视为一个在训练脚本中即时发生的、临时的内存操作。然而，一个健壮、可复现且可移植的机器学习工作流，要求我们将任何架构层面的变更“固化”下来，形成一个稳定、独立且可重载的全新基础模型。

这一原则，我们称之为**“基础模型固化”（Base Model Solidification）**。

其核心逻辑是，Hugging Face的PEFT库在设计上期望作用于一个定义明确、状态稳定的基础模型之上。当您在推理时调用`PeftModel.from_pretrained`，该函数的第一步是根据适配器配置文件（`adapter_config.json`）中的`base_model_name_or_path`字段加载基础模型。如果这个路径指向的是原始的、未经修改的模型（例如，原始的OLMoE），而该模型本身并不包含您的`CustomModule`，那么加载过程自然会失败，因为无法将适配器的权重应用到不存在的模块上。

因此，解决问题的关键在于思维模式的转变：从**“脚本化思维”转向“工件化思维”**。您不应试图“欺骗”或“绕过”PEFT库的机制来适应一个动态修改的模型。相反，您应当遵循Hugging Face `transformers`库的最佳实践，正式地创建一个新的模型“工件”（Artifact）。正如相关文档所揭示的，通过子类化`PreTrainedModel`和`PretrainedConfig`，并使用`save_pretrained`方法，您不仅仅是保存了权重，更是创建了一个版本化的、可重用的、自包含的模型资产 1。这个资产完整地封装了新的模型架构、配置以及包括

`CustomModule`初始权重在内的所有参数。

这个“固化”后的模型，不再是一个中间变量，而是您工作流第一阶段的正式产出。它成为了后续所有参数高效微调（PEFT）任务的全新、稳定的起点。一旦这个新的基础模型被正确创建和保存，后续的LoRA微tuning过程就变得与对任何标准Hugging Face模型进行微调毫无二致，从而彻底解决了您遇到的矛盾。

本报告将详细阐述如何实现这一“两阶段”工作流，涵盖从固化自定义基础模型到应用LoRA进行训练、推理和最终部署的全过程。

------



## **第一部分：基础工作流——创建并固化您的自定义基础模型**



本部分将提供一个精细化的、分步的指南，指导您如何正确地定义、创建并保存包含`CustomModule`的修改后模型，将其转化为一个可重用、独立的模型工件。这是解决您核心问题的最关键步骤。



### **2.1 定义自定义配置 (`PretrainedConfig`)**



模型的配置文件（`config.json`）是其架构的蓝图。为了让`transformers`库理解您的自定义模型，必须首先创建一个自定义的配置类，它能够序列化和反序列化模型的独特属性。

**实现细节** 1：

1. **子类化 `transformers.PretrainedConfig`**：您的自定义配置类必须继承自`PretrainedConfig`，以获得Hugging Face生态系统的所有标准功能，如`from_pretrained()`和`save_pretrained()`。
2. **定义 `model_type`**：在您的类中，设置一个名为`model_type`的类属性。它的值必须是一个唯一的字符串（例如，`"olmoe-custom"`），用于标识您的自定义架构。这个标识符对于后续将模型注册到`AutoClass`至关重要。
3. **扩展 `__init__` 方法**：自定义配置的`__init__`方法必须接受任意关键字参数（`**kwargs`），并将它们传递给父类的`__init__`方法（`super().__init__(**kwargs)`）。这样做是为了确保在加载模型时，那些非您自定义但`transformers`内部使用的配置项能够被正确处理。
4. **添加自定义参数**：在`__init__`的签名中添加您`CustomModule`所需的任何新超参数（例如，`custom_hidden_dim`），并将它们存储为实例属性。

代码示例：

假设您的自定义模型被命名为OLMoECustom，其配置文件可以这样定义。创建一个名为configuration_olmoe_custom.py的文件：

Python

```
# file: configuration_olmoe_custom.py
from transformers import PretrainedConfig

class OLMoECustomConfig(PretrainedConfig):
    """
    这是OLMoE自定义模型的配置类。
    它继承自PretrainedConfig，并添加了与CustomModule相关的特定参数。
    """
    # model_type是必须的，它将您的自定义架构与Hugging Face生态系统连接起来
    model_type = "olmoe-custom"

    def __init__(
        self,
        custom_module_dim: int = 256,
        custom_activation_function: str = "silu",
        **kwargs
    ):
        """
        初始化配置对象。

        Args:
            custom_module_dim (int, optional): CustomModule的内部维度。默认为256。
            custom_activation_function (str, optional): CustomModule使用的激活函数。默认为 "silu"。
            **kwargs: 传递给父类PretrainedConfig的额外参数。
        """
        self.custom_module_dim = custom_module_dim
        self.custom_activation_function = custom_activation_function
        
        # 必须调用super().__init__以处理所有标准参数
        super().__init__(**kwargs)
```



### **2.2 定义自定义模型架构 (`PreTrainedModel`)**



接下来，您需要编写包含`CustomModule`的实际模型架构代码。这需要确保它与`transformers`库的`PreTrainedModel`基类兼容。

**实现细节** 1：

1. **子类化 `transformers.PreTrainedModel`**：您的模型类应继承自`PreTrainedModel`或一个更具体的基类（例如，如果存在`OLMoEPreTrainedModel`，则继承自它）。这使得您的模型能够使用`.from_pretrained()`和`.save_pretrained()`等核心API。
2. **关联 `config_class`**：在模型类中，将`config_class`属性设置为您刚刚创建的自定义配置类（`OLMoECustomConfig`）。这就在模型和它的配置之间建立了明确的链接。
3. **实现 `__init__` 方法**：模型的`__init__`方法必须接受一个配置对象（即`OLMoECustomConfig`的实例）作为其第一个参数。在此方法中，您将根据配置来构建模型的各个层。
4. **集成并初始化 `CustomModule`**：这是解决您问题的关键所在。在`__init__`中，您需要：
   - 像往常一样实例化模型的标准部分。
   - 在适当的位置，用您的`CustomModule`替换掉原始模型的某个模块。
   - **关键步骤**：根据您提供的需求，`CustomModule`需要用特定的权重进行初始化。您应该在`__init__`方法内部，从指定路径加载这些权重，并应用到`CustomModule`实例上。这确保了这些重要的初始权重成为模型整体状态的一部分，并将在后续的`save_pretrained`中被一并保存。

代码示例：

创建一个名为modeling_olmoe_custom.py的文件。

Python

```
# file: modeling_olmoe_custom.py
import torch
import torch.nn as nn
import os
from transformers import PreTrainedModel
from.configuration_olmoe_custom import OLMoECustomConfig

# 假设您的CustomModule定义在另一个文件中
# from.my_custom_module_definition import CustomModule

# 为演示目的，我们在这里定义一个简单的CustomModule
class CustomModule(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.internal_linear_1 = nn.Linear(config.hidden_size, config.custom_module_dim)
        self.activation = nn.ReLU() # 假设，可根据config.custom_activation_function配置
        self.internal_linear_2 = nn.Linear(config.custom_module_dim, config.hidden_size)
    
    def forward(self, hidden_states):
        x = self.internal_linear_1(hidden_states)
        x = self.activation(x)
        x = self.internal_linear_2(x)
        return x

class OLMoECustomModel(PreTrainedModel):
    """
    一个集成了CustomModule的自定义模型。
    """
    # 将模型类与我们自定义的配置类关联起来
    config_class = OLMoECustomConfig
    
    # 定义基础模型的前缀，对于从预训练模型加载权重很重要
    base_model_prefix = "model"

    def __init__(self, config: OLMoECustomConfig):
        super().__init__(config)
        
        # 假设我们加载一个类似BERT的结构作为基础
        # 在实际情况中，这里会是OLMoE的结构
        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        
        # 这里是关键：我们用CustomModule替换了模型的一个层
        # 假设我们替换掉第一个Transformer层
        self.custom_layer = CustomModule(config)
        
        # 加载CustomModule的特定初始权重
        # 这是“固化”工作流的核心部分，确保了初始状态的完整性
        initial_weights_path = os.environ.get("CUSTOM_MODULE_WEIGHTS_PATH", "path/to/your/custom_module_weights.pth")
        if os.path.exists(initial_weights_path):
            print(f"Loading initial weights for CustomModule from {initial_weights_path}")
            # 加载权重字典，并确保它在正确的设备上
            state_dict = torch.load(initial_weights_path, map_location=self.device)
            self.custom_layer.load_state_dict(state_dict)
        else:
            print(f"Warning: Initial weights for CustomModule not found at {initial_weights_path}. Using random initialization.")

        # 实例化模型的其余部分...
        # self.remaining_layers =...
        
        # 实例化一个语言模型头
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

        # 确保新初始化的权重被正确处理
        self.post_init()

    def forward(self, input_ids, **kwargs):
        hidden_states = self.embeddings(input_ids)
        
        # 通过我们的自定义层
        hidden_states = self.custom_layer(hidden_states)
        
        # 通过模型的其余部分...
        # hidden_states = self.remaining_layers(hidden_states)
        
        logits = self.lm_head(hidden_states)
        return {"logits": logits}
```



### **2.3 将自定义架构注册到 `AutoClass` (可选但强烈推荐)**



为了让您的自定义模型能够通过便捷的`AutoModel.from_pretrained` API加载，从而提升其可用性和社区友好性，您应该将其注册到Hugging Face的`AutoClass`中。

**实现细节** 1：

1. **使用 `register` 方法**：`transformers`库提供了`AutoConfig.register()`和`AutoModelForCausalLM.register()`（或其他任务对应的`AutoModel`类）等方法。
2. **关联 `model_type`**：注册的关键是将您在自定义配置中定义的`model_type`字符串（`"olmoe-custom"`）与您的配置类和模型类关联起来。
3. **放置注册代码**：最佳实践是将注册代码放置在您的模型代码所在目录的`__init__.py`文件中。这样，当用户`import`您的模型目录时，注册会自动执行。

代码示例：

在您的模型目录（例如my_custom_model_dir）下创建一个__init__.py文件。

Python

```
# file: my_custom_model_dir/__init__.py
from.configuration_olmoe_custom import OLMoECustomConfig
from.modeling_olmoe_custom import OLMoECustomModel
from transformers import AutoConfig, AutoModelForCausalLM

# 将自定义配置注册到AutoConfig
# 第一个参数 "olmoe-custom" 必须与 OLMoECustomConfig.model_type 的值完全匹配
AutoConfig.register("olmoe-custom", OLMoECustomConfig)

# 将自定义模型注册到AutoModelForCausalLM
# 这使得 AutoModelForCausalLM.from_pretrained 能够识别和加载您的模型
AutoModelForCausalLM.register(OLMoECustomConfig, OLMoECustomModel)
```



### **2.4 保存“固化”的基础模型**



这是“固化”流程的最后一步，也是至关重要的一步。它会将您的完整模型——包括架构定义（Python代码）、配置（`config.json`）和所有权重（包括`CustomModule`的初始权重）——保存到一个本地目录中。这个目录就是我们所说的自包含的、可重用的模型工件。

**实现细节** 1：

1. **实例化模型**：首先，您需要根据一个基础模型的配置（例如，原始OLMoE的配置）来创建一个您的自定义配置和模型实例。
2. **调用 `save_pretrained`**：对模型实例调用`.save_pretrained("./my-solidified-olmoe")`。
3. **理解输出结构**：这个命令会创建一个目录，其中包含：
   - `config.json`: 序列化后的`OLMoECustomConfig`实例。
   - `model.safetensors` (或 `pytorch_model.bin`): 包含模型所有权重的二进制文件。
   - `configuration_olmoe_custom.py`: 您的自定义配置类的Python源文件。
   - `modeling_olmoe_custom.py`: 您的自定义模型类的Python源文件。
   - `__init__.py`: 包含`AutoClass`注册逻辑的Python文件。

**关键点**：`save_pretrained`会自动将相关的Python代码文件复制到保存目录中。这是为了确保当其他人或您自己在另一个环境中使用`from_pretrained`加载此模型时，`transformers`库能够找到并执行定义模型架构的代码。这也正是为什么在加载时需要`trust_remote_code=True`参数的原因 7。

代码示例：

创建一个脚本来执行固化过程。

Python

```
import torch
from transformers import AutoConfig
from my_custom_model_dir import OLMoECustomConfig, OLMoECustomModel # 确保可以导入

# 1. 加载一个基础模型的配置作为起点
base_model_name = "allenai/OLMo-1B" # 示例
base_config = AutoConfig.from_pretrained(base_model_name)

# 2. 将其转换为我们的自定义配置，并添加或修改参数
custom_config_dict = base_config.to_dict()
custom_config_dict['custom_module_dim'] = 512 # 设置我们的自定义参数
custom_config = OLMoECustomConfig.from_dict(custom_config_dict)

# 3. 使用自定义配置实例化我们的自定义模型
# 注意：此时模型权重是随机初始化的（除了我们手动加载的CustomModule权重）
# 如果需要，您可以先加载原始OLMoE的权重，再替换模块
# model = OLMoECustomModel.from_pretrained(base_model_name, config=custom_config, ignore_mismatched_sizes=True)
# 这种方式更复杂，我们这里简化为从配置初始化
model = OLMoECustomModel(custom_config) 

# 4. 保存固化模型
output_dir = "./my-solidified-olmoe"
model.save_pretrained(output_dir)

# 5. 同时保存对应的tokenizer
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
tokenizer.save_pretrained(output_dir)

print(f"固化后的自定义基础模型已保存到: {output_dir}")
```

至此，您已经成功创建了一个全新的、包含`CustomModule`及其初始权重的、可独立分发和加载的基础模型。这是进行后续所有微调工作的坚实基础。

------



## **第二部分：端到端的LoRA微调流水线**



在拥有了“固化”的自定义基础模型之后，接下来的LoRA微调流程就变得清晰且符合标准。本部分将详细介绍如何基于这个新模型工件，实施一个完整、正确的PEFT工作流。



### **3.1 加载自定义基础模型**



微调脚本的第一步是加载您在第一部分中创建的工件。

**实现细节** 2：

- **使用 `from_pretrained` 和 `trust_remote_code`**：加载模型的关键命令是`AutoModelForCausalLM.from_pretrained("./my-solidified-olmoe", trust_remote_code=True)`。
- **`trust_remote_code=True` 的必要性**：必须明确传递此参数。它授权`transformers`库执行保存在`./my-solidified-olmoe`目录中的`modeling_olmoe_custom.py`和`configuration_olmoe_custom.py`文件。没有这个许可，库将无法实例化您的自定义架构，从而导致失败。这是一个安全机制，旨在防止执行未经验证的远程代码，但在加载您自己创建的本地可信代码时，这是标准且正确的做法。

Python

```
# 在您的微调脚本中
from transformers import AutoModelForCausalLM, AutoTokenizer

model_path = "./my-solidified-olmoe"

# 加载固化后的自定义基础模型
# trust_remote_code=True 是加载自定义架构代码的关键
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16, # 推荐使用以节省内存
    device_map="auto"
)

# 加载对应的tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)
```



### **3.2 正确配置PEFT以在`CustomModule`上进行定向LoRA微调**



这是解决您最初问题的核心技术环节。目标是创建一个`LoraConfig`，它能精确地将LoRA适配器注入到`CustomModule`内部的特定层，同时保持模型其余部分（包括`CustomModule`的其他部分）的权重冻结。

**实现细节** 8：

1. **检查模块名称**：在配置LoRA之前，最可靠的方法是编程方式检查并打印出模型中所有模块的名称。这可以帮助您找到`CustomModule`内部线性层的确切路径。

   Python

   ```
   # 打印模型结构以找到目标模块的准确名称
   print(model)
   
   # 或者更详细地打印所有可命名模块
   # for name, module in model.named_modules():
   #     print(name)
   ```

   通过检查输出，您可能会找到类似`"custom_layer.internal_linear_1"`或`"model.custom_layer.internal_linear_2"`这样的名称。

2. **创建`LoraConfig`**：现在，您可以创建`LoraConfig`对象。

   - **`target_modules`**：这是最重要的参数。它的值应该是一个字符串列表，其中每个字符串都是您希望应用LoRA的线性层（或`Conv1D`等其他支持的层）的名称。根据上一步的检查，您应该填入`CustomModule`内部的层名，例如`["custom_layer.internal_linear_1", "custom_layer.internal_linear_2"]`。

   - **澄清 `target_modules` 与 `modules_to_save` 的区别**：这是您问题的关键。这两个参数服务于完全不同的目的。

     - **`target_modules`**：用于**参数高效微调 (LoRA)**。当一个模块被列入`target_modules`时，PEFT会冻结该模块的原始权重，并在其旁边注入两个小的、可训练的低秩矩阵（LoRA A和B）。训练时，只有这些小矩阵的参数被更新。这正是您想要对`CustomModule`执行的操作。

     - **`modules_to_save`**：用于**全参数微调**。当一个模块被列入`modules_to_save`时，它意味着您希望对这个模块进行传统的全参数微tuning，并将其完整的、更新后的权重与LoRA适配器一起保存。PEFT的实现方式是创建一个该模块的完整副本进行训练 10。这通常用于训练那些需要从头学习的模块，比如一个新添加的、随机初始化的分类头。在您的情况下，对

       `CustomModule`使用此选项会违背使用LoRA的初衷。

下表清晰地总结了二者的区别：

| 参数              | 目的                                                     | 训练方法            | 参数量影响                      | 典型用例                                                  |
| ----------------- | -------------------------------------------------------- | ------------------- | ------------------------------- | --------------------------------------------------------- |
| `target_modules`  | 指定需要通过LoRA进行适配的模块。                         | 参数高效微调 (LoRA) | 极小 (增加 r×(din+dout) 个参数) | 适配Transformer中的注意力投影层: `["q_proj", "v_proj"]`。 |
| `modules_to_save` | 指定需要进行全参数训练并与LoRA适配器一同保存的额外模块。 | 全参数微调          | 巨大 (训练该模块的全部原始参数) | 训练一个新添加的、随机初始化的分类头: `["classifier"]`。  |

**代码示例**：

Python

```
from peft import LoraConfig, get_peft_model, TaskType

# 假设通过检查模型结构，我们确定了CustomModule内部的线性层名称
target_layers = [
    "custom_layer.internal_linear_1",
    "custom_layer.internal_linear_2"
]

# 正确配置LoraConfig，只针对CustomModule内部的层
lora_config = LoraConfig(
    r=8,                           # LoRA的秩，一个关键超参数
    lora_alpha=16,                  # LoRA的缩放因子
    target_modules=target_layers,   # 精确指定目标层
    lora_dropout=0.05,              # LoRA层的dropout率
    bias="none",                    # 通常设置为"none"或"all"
    task_type=TaskType.CAUSAL_LM    # 指定任务类型
)

# 使用get_peft_model将LoRA适配器应用到我们的自定义模型上
# 注意：get_peft_model用于为“从头开始训练”准备适配器
peft_model = get_peft_model(model, lora_config)

# 打印可训练参数，验证配置是否正确
# 输出应该只显示LoRA相关的参数 (lora_A, lora_B) 是可训练的
peft_model.print_trainable_parameters()
# 示例输出: trainable params: 1,572,864 |

| all params: 1,253,789,696 |
| trainable%: 0.12545
```



### **3.3 使用高级框架进行训练 (例如 `TRL SFTTrainer`)**



准备好的`peft_model`可以无缝集成到如`TRL`这样的高级训练库中，极大地简化了训练过程。

**实现细节** 12：

- `SFTTrainer`是`TRL`库中用于监督微调的强大工具，它原生支持PEFT。
- 您只需将`peft_model`和`peft_config`（即`lora_config`）传递给`SFTTrainer`的构造函数。训练器会自动识别并只更新模型中可训练的参数。

**代码示例**：

Python

```
from transformers import TrainingArguments
from trl import SFTTrainer
from datasets import load_dataset # 假设您有一个数据集

# 加载数据集
dataset = load_dataset("your/dataset/name", split="train")

# 定义训练参数
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    logging_steps=10,
    num_train_epochs=3,
    save_steps=100,
    #... 其他训练参数
)

# 创建SFTTrainer实例
trainer = SFTTrainer(
    model=peft_model,               # 直接传入PEFT模型
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset,
    peft_config=lora_config,        # 传入LoRA配置
    dataset_text_field="text",      # 数据集中包含文本的列名
    max_seq_length=1024,            # 序列最大长度
)

# 开始训练
trainer.train()
```



### **3.4 保存生成的LoRA适配器**



训练完成后，您需要保存学到的LoRA权重。

**实现细节** 9：

- 调用`trainer.save_model()`或直接对模型调用`peft_model.save_pretrained()`。
- 这将在指定的输出目录中创建一个非常小的适配器。该目录通常包含：
  - `adapter_model.safetensors`: 仅包含LoRA矩阵（A和B）的权重。
  - `adapter_config.json`: 适配器的配置文件，其中最重要的键是`base_model_name_or_path`，它的值将是`"./my-solidified-olmoe"`。这记录了该适配器是为哪个基础模型训练的。

**代码示例**：

Python

```
# 训练结束后，保存适配器
adapter_output_dir = "./my-olmoe-custom-adapter"
trainer.save_model(adapter_output_dir) # 或者 peft_model.save_pretrained(adapter_output_dir)

print(f"LoRA适配器已保存到: {adapter_output_dir}")
```

通过以上步骤，您已经成功地基于一个包含自定义模块的、固化后的基础模型，完成了一次精确的、参数高效的LoRA微调，并得到了一个轻量级的适配器。

------



## **第三部分：推理与部署工作流**



在获得了针对自定义模型的LoRA适配器后，您有两种主流的部署和推理策略：动态加载和静态合并。本部分将详细介绍这两种方法的实现和适用场景。



### **4.1 动态推理：加载自定义基础模型并附加适配器**



这是最常见、最灵活的推理方法。它在运行时将基础模型和适配器分开加载，然后在内存中进行组合。这种方法允许您为同一个基础模型加载和切换多个不同的任务适配器，极大地节省了存储空间。

**实现细节** 16：

1. **加载固化的自定义基础模型**：与训练时一样，首先加载您在第一部分创建的`./my-solidified-olmoe`模型，同样需要`trust_remote_code=True`。
2. **加载并应用适配器**：使用`PeftModel.from_pretrained`方法，将训练好的适配器应用到已加载的基础模型上。

**一个至关重要的区别：`get_peft_model` vs. `PeftModel.from_pretrained`**

在实践中，一个非常容易混淆且会导致静默失败（Silent Failure）的错误是误用`get_peft_model`来加载已训练的适配器。这两个函数有明确且不同的用途 17：

- **`get_peft_model(model, config)`**：此函数用于**初始化一个新的、待训练的PEFT模型**。它会在`model`上根据`config`注入**随机初始化**的适配器层。它适用于开始一次全新的训练任务。
- **`PeftModel.from_pretrained(model, adapter_path)`**：此函数用于**加载一个已经训练好的PEFT适配器**，并将其权重应用到`model`上。它适用于推理或在已有适配器的基础上继续训练。

如果在加载已训练的适配器时错误地先调用了`get_peft_model`，您实际上会先注入一组新的随机适配器，然后再尝试加载旧的适配器，这很可能导致参数不匹配或加载的权重被忽略，使得模型表现得如同没有微调一样。

**代码示例（正确的动态推理流程）**：

Python

```
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# 路径定义
base_model_path = "./my-solidified-olmoe"
adapter_path = "./my-olmoe-custom-adapter"

# 步骤 1: 加载固化的自定义基础模型
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    trust_remote_code=True,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# 步骤 2: 加载并应用训练好的LoRA适配器
# 这里必须使用 PeftModel.from_pretrained，而不是 get_peft_model
inference_model = PeftModel.from_pretrained(base_model, adapter_path)

# 步骤 3: 准备tokenizer并进行推理
tokenizer = AutoTokenizer.from_pretrained(base_model_path)
prompt = "Your prompt here..."
inputs = tokenizer(prompt, return_tensors="pt").to(inference_model.device)

with torch.no_grad():
    outputs = inference_model.generate(**inputs, max_new_tokens=100)
    print(tokenizer.decode(outputs, skip_special_tokens=True))
```



### **4.2 创建可移植的独立模型：合并适配器**



在某些部署场景下（例如，环境限制无法安装`peft`库，或者为了极致的推理性能），您可能希望将LoRA适配器的权重永久性地合并回基础模型中，从而创建一个单一的、独立的模型工件。

**实现细节** 16：

- **使用 `merge_and_unload` 方法**：`PeftModel`对象提供了一个便捷的方法`merge_and_unload()`。
- **工作原理**：该方法会遍历所有被LoRA适配器修改过的目标模块（`target_modules`）。对于每个模块，它会计算出合并后的新权重，其数学本质是 Wmerged=Woriginal+ΔW=Woriginal+B⋅A⋅scaling。然后，它用这个$W_{merged}$替换掉原始的权重矩阵$W_{original}$。最后，它会移除所有LoRA相关的层，并返回一个标准的、不再是`PeftModel`的`transformers`模型对象。这个返回的模型内部的权重已经被永久改变。

**代码示例**：

Python

```
# 接着上一节的 inference_model (PeftModel对象)
print("Merging adapter weights into the base model...")

# 调用 merge_and_unload()
# 该方法返回一个标准的 transformers 模型，其权重已被合并
merged_model = inference_model.merge_and_unload()

print("Merge complete. The returned model is a standard Transformer model.")
# 现在 merged_model 可以像任何普通模型一样使用，不再需要peft库进行推理
```



### **4.3 保存并分发最终的合并模型**



合并后，您应该将这个独立的模型保存下来，以便于分发和部署。

**实现细节** 16：

- **使用 `save_pretrained`**：对`merged_model`调用标准的`save_pretrained`方法。
- **工件特征**：保存下来的目录将包含完整的模型权重，因此其大小与基础模型相当（即“大模型”）。但它的优势在于完全自包含：
  - 它包含了您的自定义架构代码（`.py`文件）。
  - 它包含了合并了LoRA增量的最终权重。
  - 任何人在加载它时，只需`AutoModelForCausalLM.from_pretrained(..., trust_remote_code=True)`，而**无需安装或了解PEFT库**。

**代码示例**：

Python

```
# 保存合并后的独立模型
merged_model_output_dir = "./my-final-merged-model"
merged_model.save_pretrained(merged_model_output_dir)

# 不要忘记同时保存tokenizer
tokenizer.save_pretrained(merged_model_output_dir)

print(f"Final merged model saved to: {merged_model_output_dir}")

# 后续任何人都可以这样加载和使用它，无需PEFT
# from transformers import AutoModelForCausalLM
# final_model = AutoModelForCausalLM.from_pretrained(
#     merged_model_output_dir,
#     trust_remote_code=True
# )
```

为了更好地理解整个工作流中产生的不同模型工件，下表总结了它们的生命周期和特性：

| 工件类型         | 创建方式                         | 内容                                                         | 大小          | 用途与特点                                                   |
| ---------------- | -------------------------------- | ------------------------------------------------------------ | ------------- | ------------------------------------------------------------ |
| **固化基础模型** | `model.save_pretrained()`        | 完整的模型权重（含自定义模块）、配置和Python源代码。         | 巨大 (GB级别) | 作为所有PEFT微调的、可重用的、版本化的基础。                 |
| **LoRA适配器**   | `peft_model.save_pretrained()`   | 仅包含训练好的LoRA权重（A、B矩阵）和适配器配置。             | 微小 (MB级别) | 轻量级的、可插拔的任务特定增量。用于动态推理，易于分享和管理。 |
| **合并后模型**   | `merged_model.save_pretrained()` | 完整的模型权重，已将LoRA适配器权重融合进去，以及配置和Python源代码。 | 巨大 (GB级别) | 可移植的、独立的最终模型。用于不需要或不方便使用PEFT库的部署环境。 |

这张表为您的MLOps实践提供了清晰的指引，帮助您理解在工作流的每个阶段应该管理、版本化和部署哪种类型的资产。

------



## **4. 结论：最佳实践与陷阱规避**



针对在包含自定义模块的模型上进行LoRA微调这一复杂任务，本报告提出并验证了一个基于“基础模型固化”原则的、健壮且可复现的端到端工作流。为确保您在实践中能够顺利实施，现将核心的最佳实践和需要规避的常见陷阱总结如下。



### **最佳实践清单**



- **优先固化 (Solidify First)**：在应用任何PEFT技术之前，务必将任何架构层面的修改（如添加`CustomModule`）通过子类化`PreTrainedModel`和`PretrainedConfig`的方式实现，并使用`model.save_pretrained()`将其保存为一个完整的、自包含的新基础模型。这是整个工作流的基石。
- **拥抱 `trust_remote_code`**：理解并正确使用`trust_remote_code=True`参数。它不是一个安全漏洞，而是加载包含自定义Python代码的本地或远程模型工件时，一个必需且正确的步骤。
- **精确靶向 (Target Precisely)**：在配置`LoraConfig`时，务必使用`model.named_modules()`或`print(model)`等方法，仔细检查并获取您`CustomModule`内部待训练层的确切名称，然后将这些名称字符串列表传递给`target_modules`参数。
- **工具专用 (Use the Right Tool for the Job)**：严格区分`get_peft_model`和`PeftModel.from_pretrained`的用途。
  - `get_peft_model`：用于为**新训练**注入随机初始化的适配器。
  - `PeftModel.from_pretrained`：用于为**推理或继续训练**加载已保存的适配器。混用它们是导致静默失败的常见原因 17。
- **工件管理 (Manage Your Artifacts)**：清晰地识别和管理工作流中产生的三种核心工件：固化基础模型、LoRA适配器和合并后模型。根据它们的用途、大小和依赖关系，进行恰当的版本控制、存储和部署。



### **常见陷阱规避**



- **`modules_to_save` 的误用陷阱**：切勿将`modules_to_save`用于期望进行参数高效微调的模块。此参数会触发**全参数微调**，这与LoRA的目标背道而驰，并可能导致与您的预期完全不符的训练行为和资源消耗 10。
- **`get_peft_model` 的加载错误**：绝对不要使用`get_peft_model`来加载一个之前训练并保存好的适配器。这种错误的操作不会抛出异常，但会导致模型在推理时表现得像没有加载适配器一样，极难调试 17。
- **“内存中修改”的陷阱**：避免在单个训练脚本中动态地修改模型架构，然后直接进行PEFT微调。这种“临时抱佛脚”式的方法正是导致您最初遇到的“基础模型在推理时找不到自定义模块”问题的根源。
- **遗忘自定义代码文件**：当您分享或迁移一个固化后的或合并后的自定义模型时，请确保其目录中包含了必要的Python源文件（`modeling_*.py`, `configuration_*.py`, `__init__.py`）。没有这些文件，即使设置了`trust_remote_code=True`，`transformers`库也无从知晓如何构建您的自定义模型。