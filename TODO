路由参数的初始化

垃圾专家的输出：为0，随机噪声

基础垃圾桶：为0，随机噪声，恒等映射

loss的约束：根据token长度不同，根据激活比例来看，负载均衡。

LESS是在FLAN V2、CoT、DOLLY、OPEN ASSISTANT 1的四个混合数据集上预热训练的。然后根据不同的任务去进行数据选择。例如：mmlu-chat_adam_sim_trainp0.05_seed3_p0.05，这表示其针对MMLU数据集的chat任务，使用adam优化器，模拟训练集的比例为0.05，随机种子为3。需要从什么数据集中选择数据，就在什么数据集上预热。
在LESS的阶段2，它使用了traker库来对单一训练集获取梯度存储（使用随机投影方法）
在第三步中，它需要指定源训练集和单个目标任务，从而挑选出真正的训练集。

也就是说，如果现在有训练集D1到DN，那么就要把D1到DN的所有数据集都混合得到D，并完成预热训练。
再对D1到DN都获取梯度存储。
根据不同的任务，选择不同的源训练集和目标任务，挑选出真正的训练集。

一个自然而然的疑问是：我们的方法不需要指定任务。如果是同样挑选5%的数据的话，我们会将这5%的数据用于一次训练，然后在多个任务上测试。但LESS会在每个任务上各挑选5%的数据进行训练。如果进行更公平的比较？

目前约束计算的方式是：在top-k个专家中，垃圾桶专家的概率。如果top-k个专家中是没有垃圾专家的，那么trash_can_ratio就是0，约束值最大。不过这里是否应该设置为计算logits分数softmax后得到的概率，而非只从top-k个专家中计算，这样的话即使top-k个专家没有垃圾专家也能让它得到训练。

def custom_constraint_loss(
    router_logits: List[torch.Tensor], config: SelectMoeConfig
) -> torch.Tensor:
    """
    Compute custom constraint loss for trash can experts.

    This loss encourages the model to balance between using real experts and trash can experts
    based on data quality. Higher trash can usage should correlate with lower data quality.
    """
    if len(router_logits) == 0:
        return torch.tensor(
            0.0, device=next(iter(router_logits)).device if router_logits else "cpu"
        )

    total_loss = 0.0
    num_layers = 0

    for layer_router_logits in router_logits:
        if layer_router_logits is None:
            continue

        # Get routing probabilities for all experts
        routing_probs = F.softmax(layer_router_logits, dim=-1)

        # Get top-k routing decisions (actual expert selection)
        top_k_probs, selected_experts = torch.topk(
            routing_probs, k=config.num_experts_per_tok, dim=-1
        )

        # Calculate trash can expert usage ratio
        # Trash can experts are indices >= config.num_experts
        trash_can_mask = selected_experts >= config.num_experts
        trash_can_probs = top_k_probs * trash_can_mask.float()
        trash_can_ratio = trash_can_probs.sum(dim=-1)  # Shape: (batch_size * seq_len,)


        # Beta distribution inspired loss to encourage balanced usage
        # When trash_can_ratio is too low (high quality data): loss increases
        # When trash_can_ratio is too high (low quality data): loss decreases
        alpha = config.trash_can_loss_alpha
        beta = config.trash_can_loss_beta

        # Clamp ratio to avoid log(0)
        ratio_clamped = torch.clamp(trash_can_ratio, min=1e-8, max=1.0 - 1e-8)

        if alpha == 1.0:
            # Simplified form: encourages moderate trash can usage
            layer_loss = -(beta - 1) * torch.log(ratio_clamped) - torch.log(
                1 - ratio_clamped
            )
        else:
            # Full beta distribution form
            layer_loss = -(
                (alpha - 1) * torch.log(ratio_clamped)
                + (beta - 1) * torch.log(1 - ratio_clamped)
            )

        total_loss += layer_loss.mean()
        num_layers += 1

    return total_loss / max(num_layers, 1)



LESS的训练设置：
线性预热，余弦衰减，学习率2e-5，batch size=128，epoch=4，
Lora：r=128, alpha=512, dropout=0.1, attention的所有参数，

约束的正确性
使用超参数控制损失

在一个batch内计算的约束损失是否会影响效果？


CUDA_VISIBLE_DEVICES=2 back scripts/run_stage_1.sh -m training.learning_rate=1e-3,3e-4
CUDA_VISIBLE_DEVICES=6 back scripts/run_stage_1.sh -m training.learning_rate=1e-4,3e-5

预热训练
CUDA_VISIBLE_DEVICES=2 scripts/run_stage_1.sh training.learning_rate=1e-3 dataset.subset_ratio=0.0001 training.quality_loss_type="beta_moment_matching" tag="test"

CUDA_VISIBLE_DEVICES=2 back scripts/run_stage_1.sh -m training.learning_rate=1e-3,3e-4,1e-4 training.quality_loss_type="beta_moment_matching"

CUDA_VISIBLE_DEVICES=5 back scripts/run_stage_1.sh -m training.learning_rate=1e-3,3e-4,1e-4 training.quality_loss_type="sigmoid"

CUDA_VISIBLE_DEVICES=7 back scripts/run_stage_1.sh -m training.learning_rate=1e-3,3e-4,1e-4 training.quality_loss_type="mean_variance_regularization"


CUDA_VISIBLE_DEVICES=2 back scripts/run_stage_1.sh training.learning_rate=1e-3 training.quality_loss_type="mse"
CUDA_VISIBLE_DEVICES=5 back scripts/run_stage_1.sh training.learning_rate=3e-4 training.quality_loss_type="mse"
CUDA_VISIBLE_DEVICES=7 back scripts/run_stage_1.sh training.learning_rate=1e-4 training.quality_loss_type="mse"


 
数据选择
CUDA_VISIBLE_DEVICES=3 back scripts/run_stage_2.sh model_checkpoint_path=\"outputs/stage_1_pretrain/2025-08-09/02-46-05-batch=4,lr=0.0001/full_rank_weights.pt\" tag=\"lr=1e-4\" data_process.batch_size=16

CUDA_VISIBLE_DEVICES=2 back scripts/run_stage_2.sh model_checkpoint_path=\"outputs/stage_1_pretrain/2025-08-10/03-42-54-batch=8_lr=0.001_loss=beta_moment_matching_tag=none/full_rank_weights.pt\" tag=\"loss=beta_lr=1e-3\" data_process.batch_size=32

CUDA_VISIBLE_DEVICES=2 back scripts/run_stage_2.sh -m data_process.batch_size=32 model_checkpoint_path=\"outputs/stage_1_pretrain/2025-08-10/03-42-54-batch=8_lr=0.001_loss=beta_moment_matching_tag=none/full_rank_weights.pt\",\"outputs/stage_1_pretrain/2025-08-10/03-42-54-batch=8_lr=0.0001_loss=beta_moment_matching_tag=none/full_rank_weights.pt\",\"outputs/stage_1_pretrain/2025-08-10/03-42-54-batch=8_lr=0.001_loss=mean_variance_regularization_tag=none/full_rank_weights.pt\",\"outputs/stage_1_pretrain/2025-08-10/03-42-54-batch=8_lr=0.0001_loss=mean_variance_regularization_tag=none/full_rank_weights.pt\"

CUDA_VISIBLE_DEVICES=5 back scripts/run_stage_2.sh -m data_process.batch_size=32 model_checkpoint_path=\"outputs/stage_1_pretrain/2025-08-10/03-42-54-batch=8_lr=0.001_loss=sigmoid_tag=none/full_rank_weights.pt\",\"outputs/stage_1_pretrain/2025-08-10/03-42-54-batch=8_lr=0.0001_loss=sigmoid_tag=none/full_rank_weights.pt\",\"outputs/stage_1_pretrain/2025-08-10/03-42-54-batch=8_lr=0.0003_loss=beta_moment_matching_tag=none/full_rank_weights.pt\",\"outputs/stage_1_pretrain/2025-08-10/03-42-54-batch=8_lr=0.0003_loss=mean_variance_regularization_tag=none/full_rank_weights.pt\"

CUDA_VISIBLE_DEVICES=7 back scripts/run_stage_2.sh -m data_process.batch_size=32 model_checkpoint_path=\"outputs/stage_1_pretrain/2025-08-10/03-42-54-batch=8_lr=0.0003_loss=sigmoid_tag=none/full_rank_weights.pt\",\"outputs/stage_1_pretrain/2025-08-10/14-24-36-batch=8_lr=0.001_loss=mse_tag=none/full_rank_weights.pt\",\"outputs/stage_1_pretrain/2025-08-10/14-24-36-batch=8_lr=0.0001_loss=mse_tag=none/full_rank_weights.pt\",\"outputs/stage_1_pretrain/2025-08-10/14-24-36-batch=8_lr=0.0003_loss=mse_tag=none/full_rank_weights.pt\"

CUDA_VISIBLE_DEVICES=1 uv run scripts/continue_selection.py router_data_dir=\"outputs/stage_2_selection/2025-08-11/03-52-40-batch=8_lr=0.001_loss=beta_moment_matching_tag=none/router_data\"
CUDA_VISIBLE_DEVICES=1 uv run scripts/continue_selection.py

可视化数据
uv run examples/comprehensive_analysis.py outputs/stage_2_selection/2025-08-11/03-52-40-batch=8_lr=0.0001_loss=beta_moment_matching_tag=none/router_data --aggregate-datasets

微调
CUDA_VISIBLE_DEVICES=2 back scripts/run_stage_3.sh dataset.data_path=\"outputs/stage_2_selection/2025-08-09/13-06-09-lr=3e-4/selected_data.jsonl\" tag=\"lr=3e-4\"
在两个4090上使用更小的batch_size
CUDA_VISIBLE_DEVICES=1,3 back scripts/run_stage_3.sh dataset.data_path=\"dataset/selected_data/mmlu-chat_adam_sim_trainp0.05_seed3_p0.05.jsonl\" tag=\"lr=1e-4\" training.per_device_batch_size=4

不用推土机距离，使用余弦相似度加权和，

聚类后得到簇，从簇中轮流选

一级路由改成输出为一维

sigmoid和softmax？

pretrain过程中mean要考虑有效token

CUDA_VISIBLE_DEVICES=1,3 uv run scripts/continue_selection.py clustering_params.parallel_processes=2
CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7 uv run scripts/continue_selection.py clustering_params.enable_parallel_kmeans=true clustering_params.parallel_processes=7

CUDA_VISIBLE_DEVICES=3 uv run scripts/continue_selection.py router_data_dir=\"outputs/stage_2_selection/2025-08-11/03-52-40-batch=8_lr=0.001_loss=mean_variance_regularization_tag=none/router_data\" clustering_method="hdbscan"

CUDA_VISIBLE_DEVICES=3 uv run scripts/continue_selection.py router_data_dir=\"outputs/stage_2_selection/2025-08-11/03-52-40-batch=8_lr=0.001_loss=mean_variance_regularization_tag=none/router_data\" clustering_params.auto_k=false clustering_params.k=30

优化方案：
阶段一：
1. 自定义损失的计算方案：MSE, sigmoid, BEATA, VAR, 
2. 是否要除以seq_len以达到数据层面的幅度相同？（现在为是）
阶段二：
1. 查看数据分布的特征
2. 选择合适的聚类算法 （目前聚类算法使用的是原始分数logits还是经过softmax或sigmoid计算得出的概率值？不过问题好像也不大吗？不对，问题很大，应该要统一使用概率值来计算，而不是原始分数）（已经修改了使用softmax，但是要注意提醒其防止使用两次softmax

