#!/usr/bin/env python3
# Copyright 2024 Quality-Gate Project. All rights reserved.
# Licensed under the Apache License, Version 2.0

"""
å°† Qwen3 åŸºåº§æ¨¡å‹è½¬æ¢ä¸º Quality-Gate æ¨¡å‹

åŠŸèƒ½ï¼š
1. åŠ è½½ Qwen3 é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä»…æ”¯æŒ Qwen3ï¼Œä¸æ”¯æŒ Qwen2/Qwen2.5ï¼‰
2. åˆ›å»º Quality-Gate é…ç½®å¹¶æ·»åŠ è´¨é‡é—¨æ§å±‚
3. å¤åˆ¶ Qwen3 åŸºåº§æƒé‡åˆ° Quality-Gate æ¨¡å‹
4. ä¿å­˜ä¸ºæ ‡å‡† HuggingFace æ¨¡å‹æ ¼å¼

æ³¨æ„ï¼šæ­¤è„šæœ¬ä¸“é—¨ä¸º Qwen3 è®¾è®¡ï¼Œä¸ Quality-Gate æ¨¡å‹æ¶æ„å®Œå…¨åŒ¹é…ã€‚
"""

import argparse
import os
import random
import sys

import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# è·å–é¡¹ç›®æ ¹ç›®å½•
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from src.models.quality_gate_model import (
    QualityGateConfig,
    QualityGateForCausalLM,
    register_quality_gate,
)


def set_random_seed(seed):
    """è®¾ç½®æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ä»¥ç¡®ä¿å¯é‡ç°æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def convert_and_save_model(
    model_name="Qwen/Qwen3-1.7B",
    save_path=None,
    device="cpu",
    quality_gate_init_mean=0.0,
    quality_gate_init_std=0.02,
    quality_loss_weight=1.0,
    quality_loss_type="sigmoid",
    seed=42,
):
    """
    å°† Qwen3 é¢„è®­ç»ƒæ¨¡å‹è½¬æ¢ä¸º Quality-Gate æ¨¡å‹å¹¶ä¿å­˜

    Args:
        model_name: Qwen3 æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„ï¼ˆä»…æ”¯æŒ Qwen3ï¼‰
        save_path: æœ¬åœ°ä¿å­˜è·¯å¾„
        device: ä½¿ç”¨çš„è®¾å¤‡
        quality_gate_init_mean: è´¨é‡é—¨æ§åˆå§‹åŒ–å‡å€¼
        quality_gate_init_std: è´¨é‡é—¨æ§åˆå§‹åŒ–æ ‡å‡†å·®
        quality_loss_weight: è´¨é‡æŸå¤±æƒé‡
        quality_loss_type: è´¨é‡æŸå¤±ç±»å‹ ("sigmoid", "linear", "beta")
        seed: éšæœºç§å­

    æ³¨æ„ï¼šæ­¤å‡½æ•°ä»…æ”¯æŒ Qwen3 æ¨¡å‹ï¼Œä¸æ”¯æŒ Qwen2/Qwen2.5
    """
    # è®¾ç½®éšæœºç§å­
    set_random_seed(seed)

    print("=" * 80)
    print("Qwen3 â†’ Quality-Gate æ¨¡å‹è½¬æ¢")
    print(f"æºæ¨¡å‹: {model_name}")
    print(f"è®¾å¤‡: {device}")
    print(f"éšæœºç§å­: {seed}")
    print("=" * 80)

    # æ³¨å†Œ Quality-Gate æ¨¡å‹
    register_quality_gate()
    print("âœ“ Quality-Gate æ¨¡å‹å·²æ³¨å†Œ")

    try:
        # 1. åŠ è½½åŸå§‹ Qwen3 æ¨¡å‹
        print(f"\n1. åŠ è½½ Qwen3 æ¨¡å‹: {model_name}")
        print("   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")

        original_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map=device if device != "cpu" else None,
        )
        original_config = original_model.config

        # éªŒè¯æ¨¡å‹ç±»å‹
        model_type = type(original_model).__name__
        if "Qwen2" in model_type:
            raise ValueError(f"âŒ æ£€æµ‹åˆ° Qwen2/Qwen2.5 æ¨¡å‹ ({model_type})ã€‚\n   æœ¬è„šæœ¬ä»…æ”¯æŒ Qwen3 æ¨¡å‹ã€‚\n   è¯·ä½¿ç”¨ Qwen3 ç³»åˆ—æ¨¡å‹ï¼ˆå¦‚ Qwen/Qwen3-1.7Bï¼‰ã€‚")

        print("âœ“ Qwen3 æ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"  - æ¨¡å‹ç±»å‹: {model_type}")
        print(f"  - å±‚æ•°: {original_config.num_hidden_layers}")
        print(f"  - éšè—å±‚å¤§å°: {original_config.hidden_size}")
        print(f"  - è¯æ±‡è¡¨å¤§å°: {original_config.vocab_size}")
        print(f"  - æ€»å‚æ•°é‡: {sum(p.numel() for p in original_model.parameters()):,}")

        # 2. åˆ›å»º Quality-Gate é…ç½®
        print("\n2. åˆ›å»º Quality-Gate é…ç½®...")
        quality_gate_config_dict = original_config.to_dict()
        quality_gate_config_dict.update(
            {
                "model_type": "quality_gate",
                "quality_gate_init_mean": quality_gate_init_mean,
                "quality_gate_init_std": quality_gate_init_std,
                "quality_loss_weight": quality_loss_weight,
                "quality_loss_type": quality_loss_type,
                # é»˜è®¤æŸå¤±å‚æ•°
                "sample_wise_averaging": True,
                "full_sequence_prediction": True,
                "mask_special_tokens": True,
            }
        )

        quality_gate_config = QualityGateConfig(**quality_gate_config_dict)

        print("âœ“ Quality-Gate é…ç½®åˆ›å»ºæˆåŠŸ!")
        print(f"  - è´¨é‡é—¨æ§åˆå§‹åŒ–: mean={quality_gate_init_mean}, std={quality_gate_init_std}")
        print(f"  - è´¨é‡æŸå¤±æƒé‡: {quality_loss_weight}")
        print(f"  - è´¨é‡æŸå¤±ç±»å‹: {quality_loss_type}")

        # 3. è½¬æ¢æ¨¡å‹
        print("\n3. è½¬æ¢ä¸º Quality-Gate æ¶æ„...")

        # åˆ›å»ºæ–°çš„ Quality-Gate æ¨¡å‹
        quality_gate_model = QualityGateForCausalLM(quality_gate_config)

        # å¤åˆ¶åŸå§‹æ¨¡å‹çš„æƒé‡
        original_state_dict = original_model.state_dict()
        quality_gate_state_dict = quality_gate_model.state_dict()

        copied_count = 0
        new_params = []
        missing_params = []

        for key in quality_gate_state_dict:
            if key in original_state_dict:
                # ç›´æ¥å¤åˆ¶ Qwen3 æƒé‡
                quality_gate_state_dict[key].copy_(original_state_dict[key])
                copied_count += 1
            elif "quality_gate" in key:
                # è´¨é‡é—¨æ§å‚æ•°ï¼ˆæ–°å¢çš„ï¼‰
                new_params.append(key)
            else:
                # ä¸åº”è¯¥å‘ç”Ÿï¼šQwen3 å’Œ Quality-Gate æ¶æ„åº”è¯¥å®Œå…¨åŒ¹é…
                missing_params.append(key)

        print(f"âœ“ å¤åˆ¶ Qwen3 æƒé‡: {copied_count}/{len(original_state_dict)}")
        print(f"âœ“ æ–°å¢è´¨é‡é—¨æ§å‚æ•°: {len(new_params)} ä¸ª")

        # éªŒè¯æ¶æ„åŒ¹é…
        if missing_params:
            print(f"âš ï¸  è­¦å‘Š: {len(missing_params)} ä¸ªå‚æ•°æœªæ‰¾åˆ°åŒ¹é…")
            for param in missing_params[:5]:
                print(f"    - {param}")
            if len(missing_params) > 5:
                print(f"    ... è¿˜æœ‰ {len(missing_params) - 5} ä¸ª")
            print("   è¿™å¯èƒ½è¡¨ç¤ºæ¨¡å‹æ¶æ„ä¸åŒ¹é…ï¼Œè¯·ç¡®è®¤ä½¿ç”¨çš„æ˜¯ Qwen3 æ¨¡å‹ã€‚")

        # æ˜¾ç¤ºè´¨é‡é—¨æ§å‚æ•°ç¤ºä¾‹
        if new_params:
            print(f"  è´¨é‡é—¨æ§å±‚åˆ†å¸ƒ: æ¯å±‚ 1 ä¸ªè´¨é‡é—¨æ§")
            print(f"  ç¤ºä¾‹å‚æ•°: {new_params[0]}")

        print("âœ“ Quality-Gate æ¨¡å‹åˆ›å»ºå®Œæˆ!")

        # ç§»åŠ¨åˆ°è®¾å¤‡
        if device != "cpu":
            quality_gate_model = quality_gate_model.to(device)

        # 4. éªŒè¯è½¬æ¢
        print("\n4. éªŒè¯è½¬æ¢...")

        # æµ‹è¯•å‰å‘ä¼ æ’­
        batch_size, seq_len = 1, 8
        test_input = torch.randint(0, quality_gate_config.vocab_size, (batch_size, seq_len))
        if device != "cpu":
            test_input = test_input.to(device)

        with torch.no_grad():
            outputs = quality_gate_model(test_input, output_router_logits=False)

        print("âœ“ å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ!")
        print(f"  - è¾“å‡ºå½¢çŠ¶: {outputs.logits.shape}")

        # æµ‹è¯•è´¨é‡é—¨æ§è¾“å‡º
        with torch.no_grad():
            outputs_with_router = quality_gate_model(test_input, output_router_logits=True)

        print("âœ“ è´¨é‡é—¨æ§è¾“å‡ºæµ‹è¯•æˆåŠŸ!")
        if hasattr(outputs_with_router, "router_logits") and outputs_with_router.router_logits:
            print(f"  - è´¨é‡é—¨æ§å±‚æ•°: {len(outputs_with_router.router_logits)}")
            first_layer_router = outputs_with_router.router_logits[0]
            print(f"  - è´¨é‡é—¨æ§è¾“å‡ºå½¢çŠ¶: {first_layer_router.shape} [batch, seq_len, 1]")
        else:
            raise ValueError("âŒ è´¨é‡é—¨æ§è¾“å‡ºéªŒè¯å¤±è´¥ï¼šæœªæ‰¾åˆ° router_logits")

        # 5. ä¿å­˜æ¨¡å‹
        print("\n5. ä¿å­˜è½¬æ¢åçš„æ¨¡å‹...")

        if save_path is None:
            save_path = f"./converted_models/quality_gate_{model_name.split('/')[-1]}"

        os.makedirs(save_path, exist_ok=True)

        # ä¿å­˜æ¨¡å‹å’Œé…ç½®
        quality_gate_model.save_pretrained(save_path)
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")

        # ä¿å­˜åˆ†è¯å™¨
        print("ä¿å­˜åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        tokenizer.save_pretrained(save_path)
        print("âœ“ åˆ†è¯å™¨å·²ä¿å­˜")

        # 6. æµ‹è¯•åŠ è½½ä¿å­˜çš„æ¨¡å‹
        print("\n6. æµ‹è¯•ä»ä¿å­˜è·¯å¾„åŠ è½½...")

        test_loaded_model = QualityGateForCausalLM.from_pretrained(save_path)
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ!")
        print(f"  - æ¨¡å‹ç±»å‹: {type(test_loaded_model).__name__}")
        print(f"  - é…ç½®ç±»å‹: {type(test_loaded_model.config).__name__}")

        # å¿«é€ŸåŠŸèƒ½æµ‹è¯•
        if device != "cpu":
            test_loaded_model = test_loaded_model.to(device)

        with torch.no_grad():
            test_outputs = test_loaded_model(test_input, output_router_logits=True)

        print("âœ“ åŠ è½½æ¨¡å‹å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ!")
        print(f"  - è¾“å‡ºå½¢çŠ¶: {test_outputs.logits.shape}")

        # 7. è½¬æ¢å®Œæˆ
        print("\n" + "=" * 80)
        print("ğŸ‰ Qwen3 â†’ Quality-Gate è½¬æ¢å®Œæˆ!")
        print("=" * 80)

        print(f"\nğŸ“ ä¿å­˜è·¯å¾„: {os.path.abspath(save_path)}")

        print("\nğŸ“ ä½¿ç”¨æ–¹æ³•:")
        print("   1. æ›´æ–°é…ç½®æ–‡ä»¶ (configs/stage_1_warmup.yaml):")
        print(f"      selector_model.path: '{save_path}'")
        print("")
        print("   2. è¿è¡Œè®­ç»ƒ:")
        print("      CUDA_VISIBLE_DEVICES=0,1,2,3 bash scripts/run_stage_1.sh")
        print("")
        print("   3. åŠ è½½æ¨¡å‹ä»£ç :")
        print("      from src.models.quality_gate_model import QualityGateForCausalLM, register_quality_gate")
        print("      register_quality_gate()")
        print(f"      model = QualityGateForCausalLM.from_pretrained('{save_path}')")

        print("\nâœ… è½¬æ¢å†…å®¹:")
        print(f"   âœ“ Qwen3 åŸºåº§æƒé‡: {copied_count} ä¸ªå‚æ•°")
        print(f"   âœ“ è´¨é‡é—¨æ§å±‚: {len(new_params)} ä¸ªå‚æ•°")
        print(f"   âœ“ æ€»å±‚æ•°: {quality_gate_config.num_hidden_layers} å±‚")
        print("   âœ“ å¯ç›´æ¥ç”¨äºè®­ç»ƒå’Œæ¨ç†")

        return save_path

    except Exception as e:
        print(f"\nâŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return None


def main():
    """ä¸»è½¬æ¢è„šæœ¬ - ä»…æ”¯æŒ Qwen3"""
    parser = argparse.ArgumentParser(
        description="å°† Qwen3 é¢„è®­ç»ƒæ¨¡å‹è½¬æ¢ä¸º Quality-Gate æ ¼å¼",
        epilog="æ³¨æ„ï¼šæ­¤è„šæœ¬ä»…æ”¯æŒ Qwen3 æ¨¡å‹ï¼Œä¸æ”¯æŒ Qwen2/Qwen2.5",
    )

    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--model",
        type=str,
        default="Qwen/Qwen3-1.7B",
        help="Qwen3 æ¨¡å‹åç§°æˆ–è·¯å¾„ï¼ˆä»…æ”¯æŒ Qwen3ï¼‰",
    )
    parser.add_argument(
        "--save-path",
        type=str,
        default=None,
        help="æœ¬åœ°ä¿å­˜è·¯å¾„",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cpu",
        help="ä½¿ç”¨çš„è®¾å¤‡ (cpu, cuda, cuda:0 ç­‰)",
    )

    # Quality-Gate é…ç½®
    parser.add_argument(
        "--quality-gate-init-mean",
        type=float,
        default=0.0,
        help="è´¨é‡é—¨æ§åˆå§‹åŒ–å‡å€¼",
    )
    parser.add_argument(
        "--quality-gate-init-std",
        type=float,
        default=0.02,
        help="è´¨é‡é—¨æ§åˆå§‹åŒ–æ ‡å‡†å·®",
    )
    parser.add_argument(
        "--quality-loss-weight",
        type=float,
        default=1.0,
        help="è´¨é‡æŸå¤±æƒé‡",
    )
    parser.add_argument(
        "--quality-loss-type",
        type=str,
        default="sigmoid",
        choices=["sigmoid", "linear", "beta"],
        help="è´¨é‡æŸå¤±ç±»å‹",
    )

    # éšæœºç§å­
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­ï¼ˆé»˜è®¤: 42ï¼‰",
    )

    args = parser.parse_args()

    # éªŒè¯è®¾å¤‡
    if args.device != "cpu" and not torch.cuda.is_available():
        print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU")
        args.device = "cpu"
    elif args.device.startswith("cuda:"):
        device_id = int(args.device.split(":")[1])
        if device_id >= torch.cuda.device_count():
            print(f"âš ï¸  è®¾å¤‡ {args.device} ä¸å¯ç”¨ï¼Œå›é€€åˆ° CPU")
            args.device = "cpu"

    print(f"ä½¿ç”¨è®¾å¤‡: {args.device}")
    if args.device != "cpu":
        print(f"GPU å†…å­˜: {torch.cuda.get_device_properties(args.device).total_memory / 1024**3:.1f} GB")

    # è½¬æ¢å¹¶ä¿å­˜
    result_path = convert_and_save_model(
        model_name=args.model,
        save_path=args.save_path,
        device=args.device,
        quality_gate_init_mean=args.quality_gate_init_mean,
        quality_gate_init_std=args.quality_gate_init_std,
        quality_loss_weight=args.quality_loss_weight,
        quality_loss_type=args.quality_loss_type,
        seed=args.seed,
    )

    if result_path:
        print(f"\nâœ… æˆåŠŸ! æ¨¡å‹å·²ä¿å­˜åˆ°: {result_path}")
        exit(0)
    else:
        print("\nâŒ è½¬æ¢å¤±è´¥!")
        exit(1)


if __name__ == "__main__":
    main()
